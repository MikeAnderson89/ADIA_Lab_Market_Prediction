{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-12T21:43:31.283243500Z",
     "start_time": "2023-07-12T21:43:31.264741800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-07-12T21:43:31.795003800Z",
     "start_time": "2023-07-12T21:43:31.780992400Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_pairwise(X_train, y_train):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    n_samples = X_train.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_train[i, 2:], X_train[j, 2:]])\n",
    "            ids.append([X_train[i, :2], X_train[j, :2]])\n",
    "            labels.append(1 if y_train[i] > y_train[j] else 0)\n",
    "    return np.array(pairs).astype('float32'), np.array(labels).astype('float32'), np.array(ids)\n",
    "\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"resources\") -> None:\n",
    "    X_train_orig = X_train.copy()\n",
    "    y_train_orig = y_train.copy()\n",
    "    dates = list(X_train_orig['date'].unique())\n",
    "\n",
    "    for date in dates:\n",
    "        X_train = X_train_orig[X_train_orig['date'] == date].copy()\n",
    "        y_train = y_train_orig[y_train_orig['date'] == date].copy()\n",
    "\n",
    "        X_train = np.asarray(X_train)\n",
    "\n",
    "        y_train = np.asarray(list(y_train['y']))\n",
    "\n",
    "        #Train or Update Training on Model\n",
    "        model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "\n",
    "        if model_pathname.is_file():\n",
    "            print(f\"Opened Model for Date {date}\")\n",
    "            #Load Scaler\n",
    "            with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "                scaler = pickle.load(file)\n",
    "\n",
    "            #Load PCA\n",
    "            with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "                pca = pickle.load(file)\n",
    "\n",
    "            #Scaling\n",
    "            X_train[:,2:] = scaler.transform(X_train[:,2:])\n",
    "\n",
    "            #PCA\n",
    "            pca_ids = X_train[:,:2]\n",
    "            pca_features = pca.transform(X_train[:,2:])\n",
    "            X_train_concat = np.concatenate((pca_ids, pca_features), axis=1)\n",
    "\n",
    "            #Pairwise Transformation\n",
    "            X_train_pairs, y_train_labels, X_train_ids = convert_to_pairwise(X_train_concat, y_train)\n",
    "\n",
    "            X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_train_pairs, y_train_labels, random_state=42, shuffle=True, test_size=0.3)\n",
    "\n",
    "            model = load_model(model_pathname)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_nn,\n",
    "                y_train_nn,\n",
    "                batch_size=5000,\n",
    "                epochs=10,\n",
    "                validation_data=[X_test_nn, y_test_nn],\n",
    "                callbacks=[mc, early_stopping],\n",
    "                shuffle=False,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            #Scaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train[:,2:] = scaler.fit_transform(X_train[:,2:])\n",
    "\n",
    "            #Save Scaler\n",
    "            with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "                pickle.dump(scaler, file)\n",
    "\n",
    "            #PCA\n",
    "            n_components = 40\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_ids = X_train[:,:2]\n",
    "            pca_features = pca.fit_transform(X_train[:,2:])\n",
    "            X_train_concat = np.concatenate((pca_ids, pca_features), axis=1)\n",
    "\n",
    "            #Save PCA\n",
    "            with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "                pickle.dump(pca, file)\n",
    "\n",
    "            #Pairwise Transformation\n",
    "            X_train_pairs, y_train_labels, X_train_ids = convert_to_pairwise(X_train_concat, y_train)\n",
    "\n",
    "            #Get train and test datasets\n",
    "            X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_train_pairs, y_train_labels, random_state=42, shuffle=True, test_size=0.3)\n",
    "\n",
    "            #Neural Network Model\n",
    "            mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=2,\n",
    "                verbose=0,\n",
    "                mode='auto',\n",
    "                baseline=None,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(800, activation='relu', kernel_initializer='lecun_normal', input_shape=(X_train_nn.shape[1], X_train_nn.shape[2])),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(500, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Flatten(),\n",
    "                keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "            ])\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "            model.compile(optimizer=optimizer,\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_nn,\n",
    "                y_train_nn,\n",
    "                batch_size=5000,\n",
    "                epochs=10,\n",
    "                validation_data=[X_test_nn, y_test_nn],\n",
    "                callbacks=[mc, early_stopping],\n",
    "                shuffle=False,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "            model.save(model_pathname)\n",
    "\n",
    "        print(f\"Finished training for Date {date}\")\n",
    "\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-07-12T22:10:51.258346400Z",
     "start_time": "2023-07-12T22:10:51.249916900Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "    dates = list(X_test_orig['date'].unique())\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "    for date in dates:\n",
    "        X_test = X_test_orig[X_test_orig['date'] == date]\n",
    "\n",
    "        #Dummy 'y' variable\n",
    "        X_test_date = X_test.copy()\n",
    "        X_test_date['y'] = 0\n",
    "\n",
    "        X_test = np.asarray(X_test_date.drop(columns=['y']))\n",
    "\n",
    "        y_test = np.asarray(list(X_test_date['y']))\n",
    "\n",
    "        #Load Scaler\n",
    "        with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "            scaler = pickle.load(file)\n",
    "\n",
    "        #Load PCA\n",
    "        with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "            pca = pickle.load(file)\n",
    "\n",
    "        #Load Model\n",
    "        model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "        model = load_model(model_pathname)\n",
    "\n",
    "        #Scaling\n",
    "        X_test[:,2:] = scaler.transform(X_test[:,2:])\n",
    "\n",
    "        #PCA\n",
    "        pca_ids = X_test[:,:2]\n",
    "        pca_features = pca.transform(X_test[:,2:])\n",
    "        X_test_concat = np.concatenate((pca_ids, pca_features), axis=1)\n",
    "\n",
    "        #Pairwise Transformation\n",
    "        X_test_pairs, y_test_labels, X_test_ids = convert_to_pairwise(X_test_concat, y_test)\n",
    "\n",
    "        print(f\"Predicting for Date {date} in Test\")\n",
    "        preds = model.predict(X_test_pairs, batch_size=3000)\n",
    "\n",
    "        preds_df_1 = pd.DataFrame({'id': X_test_ids[:,0,1].flatten(), 'date': X_test_ids[:,0,0].flatten(), 'value': preds.flatten()})\n",
    "\n",
    "        result = preds_df_1.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "        result = pd.merge(X_test_date, result, on=['id', 'date'], how='left')\n",
    "\n",
    "        result = result[['date', 'id', 'value']]\n",
    "\n",
    "        lower, upper = -1, 1\n",
    "        result['value'] = [lower + (upper - lower) * x for x in result['value']]\n",
    "\n",
    "        result['value'] = result['value'].fillna(0)\n",
    "\n",
    "        result_df = pd.concat([result_df, result], ignore_index=False, axis=0)\n",
    "\n",
    "        print(f\"Finished predictions for Date {date} in Test\")\n",
    "\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
