{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-04T19:49:48.996737200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T01:08:28.586178100Z",
     "start_time": "2023-08-04T01:08:28.563169500Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def convert_to_pairwise_train(X_train, y_train):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    n_samples = X_train.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_train[i, 2:], X_train[j, 2:]])\n",
    "            ids.append([X_train[i, :2], X_train[j, :2]])\n",
    "            labels.append(1 if y_train[i] > y_train[j] else 0)\n",
    "    return np.array(pairs).astype('float32'), np.array(labels).astype('float32'), np.array(ids)\n",
    "\n",
    "def convert_to_pairwise_test(X_test):\n",
    "    pairs = []\n",
    "    ids = []\n",
    "    n_samples = X_test.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_test[i, 2:], X_test[j, 2:]])\n",
    "            ids.append([X_test[i, :2], X_test[j, :2]])\n",
    "    return np.array(pairs).astype('float32'), np.array(ids)\n",
    "\n",
    "\n",
    "def pairwise_generator(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i] > batch_y[j] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator_ids(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            X_ids_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                        X_ids_batch.append(X_id_pair)\n",
    "\n",
    "            yield np.array(X_batch), np.array(X_ids_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator2(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]  # Pairs are now complete rows\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator3(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                    if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(y_pair)\n",
    "                        generated_pairs.add(pair_key)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        X_batch_ids = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    X_pair_ids = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    X_batch_ids.append(X_batch_ids)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32'), np.array(X_batch_ids)\n",
    "                        X_batch = []\n",
    "                        y_batch = []\n",
    "                        X_batch_ids = []\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Create a dictionary to store indices for each date\n",
    "        date_indices = defaultdict(list)\n",
    "        for i in range(n_samples):\n",
    "            date_indices[batch_X[i, 0]].append(i)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = date_indices[date_i]\n",
    "            for j in same_date_indices:\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                pair_key = (i, j)  # Use indices directly as a unique key\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = np.array([batch_X[i, 2:], batch_X[j, 2:]])\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "### TRAIN METHODOLOGY 2 ###\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 150\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch = len(X_train_concat) // batch_size,\n",
    "            batch_size=batch_size,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = len(X_test_concat) // batch_size,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "        ])\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=len(X_train_concat) // batch_size,\n",
    "            epochs=30,\n",
    "            validation_steps=len(X_test_concat) // batch_size,\n",
    "            validation_data=test_generator,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")\n",
    "\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:08:32.302087600Z",
     "start_time": "2023-08-04T01:08:32.291087400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Load PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    # PCA\n",
    "    pca_features = pca.transform(X_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 1000\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:08:33.243496600Z",
     "start_time": "2023-08-04T01:08:33.227497400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:05:01.017717500Z",
     "start_time": "2023-08-04T01:04:59.658561300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415591, 42)\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2, 1) vs (None,)).\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 108\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(X_train, y_train, model_directory_path)\u001B[0m\n\u001B[0;32m    102\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3e-4\u001B[39m)\n\u001B[0;32m    104\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m    105\u001B[0m                   loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    106\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 108\u001B[0m     history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_train_concat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test_concat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mmc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     model\u001B[38;5;241m.\u001B[39msave(model_pathname)\n\u001B[0;32m    126\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9j3ow0dr.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\mikea\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2, 1) vs (None,)).\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:05:06.793995900Z",
     "start_time": "2023-08-04T01:05:01.512535700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200]\n",
    "y_test = y_train[y_train['date'] == 200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:47:01.750090700Z",
     "start_time": "2023-08-03T20:47:01.726293400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "        date                                                 id         0  \\\n489422   200  b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de... -0.395584   \n489423   200  77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...  1.560541   \n489424   200  f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1... -0.004308   \n489425   200  15e3163819739dc4b9318670fbda0c7361b25dc31b3025...  1.032348   \n489426   200  b61439cc04e2a6666b7b3834124952b00742885c74b4a3...  0.764759   \n...      ...                                                ...       ...   \n492839   200  ebcc110654e89b43d198347ffdf95155a29dd820cba947...  0.745428   \n492840   200  ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb... -0.961760   \n492841   200  92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad... -2.151686   \n492842   200  2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a... -1.108856   \n492843   200  054ba9c0ace464311f89cbda16873a29566ff120e9c8bd... -0.878197   \n\n               1         2         3         4         5         6         7  \\\n489422  0.467767  0.334684 -0.003670 -0.084305 -0.610649 -1.911305  1.869958   \n489423 -0.345553  0.395517  0.538143  0.061281 -0.426155 -0.099393  0.442215   \n489424  1.535068 -0.351770 -0.574043 -0.736015 -0.483100  1.136352  0.610554   \n489425 -0.108589  0.112654 -0.984792  0.000401 -0.497814  0.447101 -0.370030   \n489426 -0.020000 -0.174101  0.250532 -0.470467  1.320027  0.844867 -0.067772   \n...          ...       ...       ...       ...       ...       ...       ...   \n492839  0.440497 -0.381767  1.300897  0.678993 -1.070824 -0.163306  0.051261   \n492840 -0.365210 -2.539697  2.138631  0.538042 -0.479963  0.536242 -0.829076   \n492841  2.589724  0.579317 -0.722497 -0.456387 -0.235593 -1.049381  0.218806   \n492842  0.340626  1.462280  0.606334 -0.547911 -0.259995  0.471951  1.366694   \n492843 -0.467488  1.553955  1.128971  1.562995 -1.405781 -1.011634 -1.937196   \n\n        ...       451       452       453       454       455       456  \\\n489422  ...  0.072215  0.064613  0.063679 -0.942424  0.437068  0.162571   \n489423  ...  2.041005 -1.091513 -0.949481  0.495675  0.373371 -0.370041   \n489424  ... -0.076245  0.613645 -0.076734  0.458068 -1.385604 -1.153192   \n489425  ...  0.302309 -0.917909  0.796511 -1.143900 -0.840966 -1.124376   \n489426  ...  0.526456 -1.779329 -0.287168  1.196917  0.474941  0.009121   \n...     ...       ...       ...       ...       ...       ...       ...   \n492839  ... -0.479455 -1.193991  0.057748 -0.266845 -1.978869  0.173025   \n492840  ... -0.181034 -0.642921  1.288435 -0.998922 -0.983312  0.108591   \n492841  ... -0.037265  1.733668  0.443702  0.438292  0.559468 -1.700752   \n492842  ...  0.862505  0.735640 -1.822171 -0.510724 -2.559841 -0.556591   \n492843  ... -0.821479 -0.165956 -0.031798  0.334694 -1.616378 -0.463422   \n\n             457       458       459       460  \n489422 -0.511908  0.700700  0.531277  0.554467  \n489423  1.043447 -0.436087 -1.206550  0.870037  \n489424  2.216623  1.369278 -0.214589  1.349027  \n489425 -0.075706  0.875465 -0.645114  0.108495  \n489426 -0.285514 -1.211052  3.517855 -1.681823  \n...          ...       ...       ...       ...  \n492839  0.636986 -0.145487 -1.612342  0.670242  \n492840 -0.109811  0.109938 -1.127231  0.548930  \n492841 -2.230316 -0.355390  0.310145 -1.594635  \n492842  1.892561  1.818054 -0.408390  0.190328  \n492843  1.549226  0.209962  0.020122  0.301778  \n\n[3422 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>489422</th>\n      <td>200</td>\n      <td>b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de...</td>\n      <td>-0.395584</td>\n      <td>0.467767</td>\n      <td>0.334684</td>\n      <td>-0.003670</td>\n      <td>-0.084305</td>\n      <td>-0.610649</td>\n      <td>-1.911305</td>\n      <td>1.869958</td>\n      <td>...</td>\n      <td>0.072215</td>\n      <td>0.064613</td>\n      <td>0.063679</td>\n      <td>-0.942424</td>\n      <td>0.437068</td>\n      <td>0.162571</td>\n      <td>-0.511908</td>\n      <td>0.700700</td>\n      <td>0.531277</td>\n      <td>0.554467</td>\n    </tr>\n    <tr>\n      <th>489423</th>\n      <td>200</td>\n      <td>77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...</td>\n      <td>1.560541</td>\n      <td>-0.345553</td>\n      <td>0.395517</td>\n      <td>0.538143</td>\n      <td>0.061281</td>\n      <td>-0.426155</td>\n      <td>-0.099393</td>\n      <td>0.442215</td>\n      <td>...</td>\n      <td>2.041005</td>\n      <td>-1.091513</td>\n      <td>-0.949481</td>\n      <td>0.495675</td>\n      <td>0.373371</td>\n      <td>-0.370041</td>\n      <td>1.043447</td>\n      <td>-0.436087</td>\n      <td>-1.206550</td>\n      <td>0.870037</td>\n    </tr>\n    <tr>\n      <th>489424</th>\n      <td>200</td>\n      <td>f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1...</td>\n      <td>-0.004308</td>\n      <td>1.535068</td>\n      <td>-0.351770</td>\n      <td>-0.574043</td>\n      <td>-0.736015</td>\n      <td>-0.483100</td>\n      <td>1.136352</td>\n      <td>0.610554</td>\n      <td>...</td>\n      <td>-0.076245</td>\n      <td>0.613645</td>\n      <td>-0.076734</td>\n      <td>0.458068</td>\n      <td>-1.385604</td>\n      <td>-1.153192</td>\n      <td>2.216623</td>\n      <td>1.369278</td>\n      <td>-0.214589</td>\n      <td>1.349027</td>\n    </tr>\n    <tr>\n      <th>489425</th>\n      <td>200</td>\n      <td>15e3163819739dc4b9318670fbda0c7361b25dc31b3025...</td>\n      <td>1.032348</td>\n      <td>-0.108589</td>\n      <td>0.112654</td>\n      <td>-0.984792</td>\n      <td>0.000401</td>\n      <td>-0.497814</td>\n      <td>0.447101</td>\n      <td>-0.370030</td>\n      <td>...</td>\n      <td>0.302309</td>\n      <td>-0.917909</td>\n      <td>0.796511</td>\n      <td>-1.143900</td>\n      <td>-0.840966</td>\n      <td>-1.124376</td>\n      <td>-0.075706</td>\n      <td>0.875465</td>\n      <td>-0.645114</td>\n      <td>0.108495</td>\n    </tr>\n    <tr>\n      <th>489426</th>\n      <td>200</td>\n      <td>b61439cc04e2a6666b7b3834124952b00742885c74b4a3...</td>\n      <td>0.764759</td>\n      <td>-0.020000</td>\n      <td>-0.174101</td>\n      <td>0.250532</td>\n      <td>-0.470467</td>\n      <td>1.320027</td>\n      <td>0.844867</td>\n      <td>-0.067772</td>\n      <td>...</td>\n      <td>0.526456</td>\n      <td>-1.779329</td>\n      <td>-0.287168</td>\n      <td>1.196917</td>\n      <td>0.474941</td>\n      <td>0.009121</td>\n      <td>-0.285514</td>\n      <td>-1.211052</td>\n      <td>3.517855</td>\n      <td>-1.681823</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>492839</th>\n      <td>200</td>\n      <td>ebcc110654e89b43d198347ffdf95155a29dd820cba947...</td>\n      <td>0.745428</td>\n      <td>0.440497</td>\n      <td>-0.381767</td>\n      <td>1.300897</td>\n      <td>0.678993</td>\n      <td>-1.070824</td>\n      <td>-0.163306</td>\n      <td>0.051261</td>\n      <td>...</td>\n      <td>-0.479455</td>\n      <td>-1.193991</td>\n      <td>0.057748</td>\n      <td>-0.266845</td>\n      <td>-1.978869</td>\n      <td>0.173025</td>\n      <td>0.636986</td>\n      <td>-0.145487</td>\n      <td>-1.612342</td>\n      <td>0.670242</td>\n    </tr>\n    <tr>\n      <th>492840</th>\n      <td>200</td>\n      <td>ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb...</td>\n      <td>-0.961760</td>\n      <td>-0.365210</td>\n      <td>-2.539697</td>\n      <td>2.138631</td>\n      <td>0.538042</td>\n      <td>-0.479963</td>\n      <td>0.536242</td>\n      <td>-0.829076</td>\n      <td>...</td>\n      <td>-0.181034</td>\n      <td>-0.642921</td>\n      <td>1.288435</td>\n      <td>-0.998922</td>\n      <td>-0.983312</td>\n      <td>0.108591</td>\n      <td>-0.109811</td>\n      <td>0.109938</td>\n      <td>-1.127231</td>\n      <td>0.548930</td>\n    </tr>\n    <tr>\n      <th>492841</th>\n      <td>200</td>\n      <td>92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad...</td>\n      <td>-2.151686</td>\n      <td>2.589724</td>\n      <td>0.579317</td>\n      <td>-0.722497</td>\n      <td>-0.456387</td>\n      <td>-0.235593</td>\n      <td>-1.049381</td>\n      <td>0.218806</td>\n      <td>...</td>\n      <td>-0.037265</td>\n      <td>1.733668</td>\n      <td>0.443702</td>\n      <td>0.438292</td>\n      <td>0.559468</td>\n      <td>-1.700752</td>\n      <td>-2.230316</td>\n      <td>-0.355390</td>\n      <td>0.310145</td>\n      <td>-1.594635</td>\n    </tr>\n    <tr>\n      <th>492842</th>\n      <td>200</td>\n      <td>2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a...</td>\n      <td>-1.108856</td>\n      <td>0.340626</td>\n      <td>1.462280</td>\n      <td>0.606334</td>\n      <td>-0.547911</td>\n      <td>-0.259995</td>\n      <td>0.471951</td>\n      <td>1.366694</td>\n      <td>...</td>\n      <td>0.862505</td>\n      <td>0.735640</td>\n      <td>-1.822171</td>\n      <td>-0.510724</td>\n      <td>-2.559841</td>\n      <td>-0.556591</td>\n      <td>1.892561</td>\n      <td>1.818054</td>\n      <td>-0.408390</td>\n      <td>0.190328</td>\n    </tr>\n    <tr>\n      <th>492843</th>\n      <td>200</td>\n      <td>054ba9c0ace464311f89cbda16873a29566ff120e9c8bd...</td>\n      <td>-0.878197</td>\n      <td>-0.467488</td>\n      <td>1.553955</td>\n      <td>1.128971</td>\n      <td>1.562995</td>\n      <td>-1.405781</td>\n      <td>-1.011634</td>\n      <td>-1.937196</td>\n      <td>...</td>\n      <td>-0.821479</td>\n      <td>-0.165956</td>\n      <td>-0.031798</td>\n      <td>0.334694</td>\n      <td>-1.616378</td>\n      <td>-0.463422</td>\n      <td>1.549226</td>\n      <td>0.209962</td>\n      <td>0.020122</td>\n      <td>0.301778</td>\n    </tr>\n  </tbody>\n</table>\n<p>3422 rows × 463 columns</p>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:47:02.543965600Z",
     "start_time": "2023-08-03T20:47:02.512466200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for Test Data\n",
      "182917/182917 [==============================] - 320s 2ms/step\n",
      "Finished predicting Test Data\n"
     ]
    }
   ],
   "source": [
    "test = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T20:12:55.718270900Z",
     "start_time": "2023-08-03T20:06:34.399322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39msummary()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:08:23.732196600Z",
     "start_time": "2023-08-04T01:08:23.685411200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:28.605288500Z",
     "start_time": "2023-08-04T01:27:26.503437100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415591, 42)\n",
      "Epoch 1/30\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5211\n",
      "Epoch 1: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 8ms/step - loss: 0.6929 - accuracy: 0.5218 - val_loss: 0.6965 - val_accuracy: 0.3377\n",
      "Epoch 2/30\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.4469\n",
      "Epoch 2: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6939 - accuracy: 0.4506 - val_loss: 0.6957 - val_accuracy: 0.0293\n",
      "Epoch 3/30\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4631\n",
      "Epoch 3: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6933 - accuracy: 0.4655 - val_loss: 0.6942 - val_accuracy: 0.2197\n",
      "Epoch 4/30\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5559\n",
      "Epoch 4: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6927 - accuracy: 0.5539 - val_loss: 0.6987 - val_accuracy: 0.1960\n",
      "Epoch 5/30\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5022\n",
      "Epoch 5: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6913 - val_accuracy: 0.5983\n",
      "Epoch 6/30\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5024\n",
      "Epoch 6: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "Epoch 7/30\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5475\n",
      "Epoch 7: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.6922 - accuracy: 0.5462 - val_loss: 0.6979 - val_accuracy: 0.3483\n",
      "Epoch 8/30\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.4721\n",
      "Epoch 8: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6940 - accuracy: 0.4732 - val_loss: 0.6939 - val_accuracy: 0.4663\n",
      "Epoch 9/30\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5287\n",
      "Epoch 9: saving model to ..\\resources\\model.keras\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.6927 - accuracy: 0.5269 - val_loss: 0.6898 - val_accuracy: 0.6257\n",
      "Epoch 10/30\n",
      " 52/277 [====>.........................] - ETA: 1s - loss: 0.6913 - accuracy: 0.5712"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 59\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_pathname\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m     52\u001B[0m     early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(\n\u001B[0;32m     53\u001B[0m         monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     54\u001B[0m         patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,\n\u001B[0;32m     55\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     56\u001B[0m         mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     57\u001B[0m         baseline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 59\u001B[0m     history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_train_concat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test_concat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mmc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;66;03m#Neural Network Model\u001B[39;00m\n\u001B[0;32m     74\u001B[0m     mc \u001B[38;5;241m=\u001B[39m ModelCheckpoint(model_pathname, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1569\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1567\u001B[0m \u001B[38;5;66;03m# No error, now safe to assign to logs.\u001B[39;00m\n\u001B[0;32m   1568\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[1;32m-> 1569\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m \u001B[43mdata_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_increment\u001B[49m\n\u001B[0;32m   1570\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(end_step, logs)\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1394\u001B[0m, in \u001B[0;36mDataHandler.step_increment\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1391\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m steps_remaining\n\u001B[0;32m   1392\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution\u001B[38;5;241m.\u001B[39massign(original_spe)\n\u001B[1;32m-> 1394\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m   1395\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_increment\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1396\u001B[0m     \u001B[38;5;124;03m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001B[39;00m\n\u001B[0;32m   1397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_increment\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "max_date = X_train['date'].max()\n",
    "model_directory_path = '../resources'\n",
    "min_date = 150\n",
    "X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "X_test = X_train[X_train['date'] == max_date]\n",
    "y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "#PCA\n",
    "n_components = 40\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_features = pca.fit_transform(X_scale_pca)\n",
    "X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "pca_features_test = pca.transform(X_test_scale_pca)\n",
    "X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "#Save out Scaler and PCA\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "batch_size = 1500\n",
    "train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "#Model Training\n",
    "model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "if model_pathname.is_file():\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch = len(X_train_concat) // batch_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=30,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps = len(X_test_concat) // batch_size,\n",
    "        callbacks=[mc, early_stopping],\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "else:\n",
    "    #Neural Network Model\n",
    "    mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "        # keras.layers.BatchNormalization(),\n",
    "        # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "    ])\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=len(X_train_concat) // batch_size,\n",
    "        epochs=30,\n",
    "        validation_steps=len(X_test_concat) // batch_size,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[mc, early_stopping],\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=False,\n",
    "        verbose=1\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:12.645592900Z",
     "start_time": "2023-08-04T01:26:50.259024500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 2, 40)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pairs, y_pairs = next(train_generator)\n",
    "X_pairs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:05:45.384248500Z",
     "start_time": "2023-08-04T01:05:45.132795400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pairs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:06:10.948924300Z",
     "start_time": "2023-08-04T01:06:10.942924600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:16.394465300Z",
     "start_time": "2023-08-04T01:27:16.380309300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "test_x, test_y = next(train_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:43:07.148504600Z",
     "start_time": "2023-08-03T14:43:07.131005400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200]\n",
    "y_test = y_train[y_train['date'] == 200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:32.064055500Z",
     "start_time": "2023-08-04T01:27:32.031893800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "        date                                                 id         0  \\\n489422   200  b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de... -0.395584   \n489423   200  77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...  1.560541   \n489424   200  f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1... -0.004308   \n489425   200  15e3163819739dc4b9318670fbda0c7361b25dc31b3025...  1.032348   \n489426   200  b61439cc04e2a6666b7b3834124952b00742885c74b4a3...  0.764759   \n...      ...                                                ...       ...   \n492839   200  ebcc110654e89b43d198347ffdf95155a29dd820cba947...  0.745428   \n492840   200  ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb... -0.961760   \n492841   200  92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad... -2.151686   \n492842   200  2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a... -1.108856   \n492843   200  054ba9c0ace464311f89cbda16873a29566ff120e9c8bd... -0.878197   \n\n               1         2         3         4         5         6         7  \\\n489422  0.467767  0.334684 -0.003670 -0.084305 -0.610649 -1.911305  1.869958   \n489423 -0.345553  0.395517  0.538143  0.061281 -0.426155 -0.099393  0.442215   \n489424  1.535068 -0.351770 -0.574043 -0.736015 -0.483100  1.136352  0.610554   \n489425 -0.108589  0.112654 -0.984792  0.000401 -0.497814  0.447101 -0.370030   \n489426 -0.020000 -0.174101  0.250532 -0.470467  1.320027  0.844867 -0.067772   \n...          ...       ...       ...       ...       ...       ...       ...   \n492839  0.440497 -0.381767  1.300897  0.678993 -1.070824 -0.163306  0.051261   \n492840 -0.365210 -2.539697  2.138631  0.538042 -0.479963  0.536242 -0.829076   \n492841  2.589724  0.579317 -0.722497 -0.456387 -0.235593 -1.049381  0.218806   \n492842  0.340626  1.462280  0.606334 -0.547911 -0.259995  0.471951  1.366694   \n492843 -0.467488  1.553955  1.128971  1.562995 -1.405781 -1.011634 -1.937196   \n\n        ...       451       452       453       454       455       456  \\\n489422  ...  0.072215  0.064613  0.063679 -0.942424  0.437068  0.162571   \n489423  ...  2.041005 -1.091513 -0.949481  0.495675  0.373371 -0.370041   \n489424  ... -0.076245  0.613645 -0.076734  0.458068 -1.385604 -1.153192   \n489425  ...  0.302309 -0.917909  0.796511 -1.143900 -0.840966 -1.124376   \n489426  ...  0.526456 -1.779329 -0.287168  1.196917  0.474941  0.009121   \n...     ...       ...       ...       ...       ...       ...       ...   \n492839  ... -0.479455 -1.193991  0.057748 -0.266845 -1.978869  0.173025   \n492840  ... -0.181034 -0.642921  1.288435 -0.998922 -0.983312  0.108591   \n492841  ... -0.037265  1.733668  0.443702  0.438292  0.559468 -1.700752   \n492842  ...  0.862505  0.735640 -1.822171 -0.510724 -2.559841 -0.556591   \n492843  ... -0.821479 -0.165956 -0.031798  0.334694 -1.616378 -0.463422   \n\n             457       458       459       460  \n489422 -0.511908  0.700700  0.531277  0.554467  \n489423  1.043447 -0.436087 -1.206550  0.870037  \n489424  2.216623  1.369278 -0.214589  1.349027  \n489425 -0.075706  0.875465 -0.645114  0.108495  \n489426 -0.285514 -1.211052  3.517855 -1.681823  \n...          ...       ...       ...       ...  \n492839  0.636986 -0.145487 -1.612342  0.670242  \n492840 -0.109811  0.109938 -1.127231  0.548930  \n492841 -2.230316 -0.355390  0.310145 -1.594635  \n492842  1.892561  1.818054 -0.408390  0.190328  \n492843  1.549226  0.209962  0.020122  0.301778  \n\n[3422 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>489422</th>\n      <td>200</td>\n      <td>b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de...</td>\n      <td>-0.395584</td>\n      <td>0.467767</td>\n      <td>0.334684</td>\n      <td>-0.003670</td>\n      <td>-0.084305</td>\n      <td>-0.610649</td>\n      <td>-1.911305</td>\n      <td>1.869958</td>\n      <td>...</td>\n      <td>0.072215</td>\n      <td>0.064613</td>\n      <td>0.063679</td>\n      <td>-0.942424</td>\n      <td>0.437068</td>\n      <td>0.162571</td>\n      <td>-0.511908</td>\n      <td>0.700700</td>\n      <td>0.531277</td>\n      <td>0.554467</td>\n    </tr>\n    <tr>\n      <th>489423</th>\n      <td>200</td>\n      <td>77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...</td>\n      <td>1.560541</td>\n      <td>-0.345553</td>\n      <td>0.395517</td>\n      <td>0.538143</td>\n      <td>0.061281</td>\n      <td>-0.426155</td>\n      <td>-0.099393</td>\n      <td>0.442215</td>\n      <td>...</td>\n      <td>2.041005</td>\n      <td>-1.091513</td>\n      <td>-0.949481</td>\n      <td>0.495675</td>\n      <td>0.373371</td>\n      <td>-0.370041</td>\n      <td>1.043447</td>\n      <td>-0.436087</td>\n      <td>-1.206550</td>\n      <td>0.870037</td>\n    </tr>\n    <tr>\n      <th>489424</th>\n      <td>200</td>\n      <td>f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1...</td>\n      <td>-0.004308</td>\n      <td>1.535068</td>\n      <td>-0.351770</td>\n      <td>-0.574043</td>\n      <td>-0.736015</td>\n      <td>-0.483100</td>\n      <td>1.136352</td>\n      <td>0.610554</td>\n      <td>...</td>\n      <td>-0.076245</td>\n      <td>0.613645</td>\n      <td>-0.076734</td>\n      <td>0.458068</td>\n      <td>-1.385604</td>\n      <td>-1.153192</td>\n      <td>2.216623</td>\n      <td>1.369278</td>\n      <td>-0.214589</td>\n      <td>1.349027</td>\n    </tr>\n    <tr>\n      <th>489425</th>\n      <td>200</td>\n      <td>15e3163819739dc4b9318670fbda0c7361b25dc31b3025...</td>\n      <td>1.032348</td>\n      <td>-0.108589</td>\n      <td>0.112654</td>\n      <td>-0.984792</td>\n      <td>0.000401</td>\n      <td>-0.497814</td>\n      <td>0.447101</td>\n      <td>-0.370030</td>\n      <td>...</td>\n      <td>0.302309</td>\n      <td>-0.917909</td>\n      <td>0.796511</td>\n      <td>-1.143900</td>\n      <td>-0.840966</td>\n      <td>-1.124376</td>\n      <td>-0.075706</td>\n      <td>0.875465</td>\n      <td>-0.645114</td>\n      <td>0.108495</td>\n    </tr>\n    <tr>\n      <th>489426</th>\n      <td>200</td>\n      <td>b61439cc04e2a6666b7b3834124952b00742885c74b4a3...</td>\n      <td>0.764759</td>\n      <td>-0.020000</td>\n      <td>-0.174101</td>\n      <td>0.250532</td>\n      <td>-0.470467</td>\n      <td>1.320027</td>\n      <td>0.844867</td>\n      <td>-0.067772</td>\n      <td>...</td>\n      <td>0.526456</td>\n      <td>-1.779329</td>\n      <td>-0.287168</td>\n      <td>1.196917</td>\n      <td>0.474941</td>\n      <td>0.009121</td>\n      <td>-0.285514</td>\n      <td>-1.211052</td>\n      <td>3.517855</td>\n      <td>-1.681823</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>492839</th>\n      <td>200</td>\n      <td>ebcc110654e89b43d198347ffdf95155a29dd820cba947...</td>\n      <td>0.745428</td>\n      <td>0.440497</td>\n      <td>-0.381767</td>\n      <td>1.300897</td>\n      <td>0.678993</td>\n      <td>-1.070824</td>\n      <td>-0.163306</td>\n      <td>0.051261</td>\n      <td>...</td>\n      <td>-0.479455</td>\n      <td>-1.193991</td>\n      <td>0.057748</td>\n      <td>-0.266845</td>\n      <td>-1.978869</td>\n      <td>0.173025</td>\n      <td>0.636986</td>\n      <td>-0.145487</td>\n      <td>-1.612342</td>\n      <td>0.670242</td>\n    </tr>\n    <tr>\n      <th>492840</th>\n      <td>200</td>\n      <td>ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb...</td>\n      <td>-0.961760</td>\n      <td>-0.365210</td>\n      <td>-2.539697</td>\n      <td>2.138631</td>\n      <td>0.538042</td>\n      <td>-0.479963</td>\n      <td>0.536242</td>\n      <td>-0.829076</td>\n      <td>...</td>\n      <td>-0.181034</td>\n      <td>-0.642921</td>\n      <td>1.288435</td>\n      <td>-0.998922</td>\n      <td>-0.983312</td>\n      <td>0.108591</td>\n      <td>-0.109811</td>\n      <td>0.109938</td>\n      <td>-1.127231</td>\n      <td>0.548930</td>\n    </tr>\n    <tr>\n      <th>492841</th>\n      <td>200</td>\n      <td>92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad...</td>\n      <td>-2.151686</td>\n      <td>2.589724</td>\n      <td>0.579317</td>\n      <td>-0.722497</td>\n      <td>-0.456387</td>\n      <td>-0.235593</td>\n      <td>-1.049381</td>\n      <td>0.218806</td>\n      <td>...</td>\n      <td>-0.037265</td>\n      <td>1.733668</td>\n      <td>0.443702</td>\n      <td>0.438292</td>\n      <td>0.559468</td>\n      <td>-1.700752</td>\n      <td>-2.230316</td>\n      <td>-0.355390</td>\n      <td>0.310145</td>\n      <td>-1.594635</td>\n    </tr>\n    <tr>\n      <th>492842</th>\n      <td>200</td>\n      <td>2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a...</td>\n      <td>-1.108856</td>\n      <td>0.340626</td>\n      <td>1.462280</td>\n      <td>0.606334</td>\n      <td>-0.547911</td>\n      <td>-0.259995</td>\n      <td>0.471951</td>\n      <td>1.366694</td>\n      <td>...</td>\n      <td>0.862505</td>\n      <td>0.735640</td>\n      <td>-1.822171</td>\n      <td>-0.510724</td>\n      <td>-2.559841</td>\n      <td>-0.556591</td>\n      <td>1.892561</td>\n      <td>1.818054</td>\n      <td>-0.408390</td>\n      <td>0.190328</td>\n    </tr>\n    <tr>\n      <th>492843</th>\n      <td>200</td>\n      <td>054ba9c0ace464311f89cbda16873a29566ff120e9c8bd...</td>\n      <td>-0.878197</td>\n      <td>-0.467488</td>\n      <td>1.553955</td>\n      <td>1.128971</td>\n      <td>1.562995</td>\n      <td>-1.405781</td>\n      <td>-1.011634</td>\n      <td>-1.937196</td>\n      <td>...</td>\n      <td>-0.821479</td>\n      <td>-0.165956</td>\n      <td>-0.031798</td>\n      <td>0.334694</td>\n      <td>-1.616378</td>\n      <td>-0.463422</td>\n      <td>1.549226</td>\n      <td>0.209962</td>\n      <td>0.020122</td>\n      <td>0.301778</td>\n    </tr>\n  </tbody>\n</table>\n<p>3422 rows × 463 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:33.460794Z",
     "start_time": "2023-08-04T01:27:33.423526600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3422, 42)\n",
      "(5853331, 2, 40)\n",
      "(5853331, 2, 2)\n",
      "Predicting for Test Data\n",
      "182917/182917 [==============================] - 324s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test_orig = X_test.copy()\n",
    "model_directory_path = '../resources'\n",
    "\n",
    "# Load Scaler\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Load PCA\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "# Scaling\n",
    "X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "# PCA\n",
    "pca_features = pca.transform(X_scale_pca)\n",
    "X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "# Load Model\n",
    "model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "model = load_model(model_pathname)\n",
    "\n",
    "# Pairwise Transformation using the pairwise generator\n",
    "batch_size = 1000\n",
    "print(X_test_concat.shape)\n",
    "X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test_ids.shape)\n",
    "\n",
    "print(\"Predicting for Test Data\")\n",
    "preds = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:34:27.981823600Z",
     "start_time": "2023-08-04T01:27:37.556551900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:46:46.999054900Z",
     "start_time": "2023-08-04T01:46:46.894826300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                            value\ndate id                                                          \n200  0015fa8023c5a2dc26aa87db2f08ec22f81cf6c61f2bcd9...  0.493242\n     001928d043ab0d61e39b427167ba901e23d285dedf7b0bb...  0.493242\n     001f30cfd62dbb5e7b1276d8a276020b02dce7a92f87fbb...  0.493242\n     00265585a914d27805c5a8eb6553c8329b746e461b8ea37...  0.493242\n     0026c90075906a177273f76e8ab9fee6b68673d375039c8...  0.493242\n...                                                           ...\n     ffc43b32123546b772e465d21512cd89ea97fc7063537f6...  0.493242\n     ffd0a94ebb8f96e980096862bbc838900ddf7859b342096...  0.493242\n     ffe02c37d359764065d96d007255b4b27e34fe8f81cb476...  0.493242\n     ffe43c8362b114c35887ce0a2dda3a6ffef413856e854fa...  0.493242\n     fff67c4c49047e7bd7c00ecb08add2145a35cf77af20c9b...  0.493242\n\n[3421 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"11\" valign=\"top\">200</th>\n      <th>0015fa8023c5a2dc26aa87db2f08ec22f81cf6c61f2bcd924c4c9977592fdeab</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>001928d043ab0d61e39b427167ba901e23d285dedf7b0bb311baeddfd34b3bbd</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>001f30cfd62dbb5e7b1276d8a276020b02dce7a92f87fbb74ce4a043358411b7</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>00265585a914d27805c5a8eb6553c8329b746e461b8ea37ea371aaf9adc7a436</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>0026c90075906a177273f76e8ab9fee6b68673d375039c8bb489a70820bb305a</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>ffc43b32123546b772e465d21512cd89ea97fc7063537f60690993b58d92fb34</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>ffd0a94ebb8f96e980096862bbc838900ddf7859b342096beb7ffb6fab9e934a</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>ffe02c37d359764065d96d007255b4b27e34fe8f81cb4766a6dcf6f2ba40d799</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>ffe43c8362b114c35887ce0a2dda3a6ffef413856e854fae8a2151fac2321c58</th>\n      <td>0.493242</td>\n    </tr>\n    <tr>\n      <th>fff67c4c49047e7bd7c00ecb08add2145a35cf77af20c9b2ccfcd5828daa1782</th>\n      <td>0.493242</td>\n    </tr>\n  </tbody>\n</table>\n<p>3421 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.groupby(['date', 'id']).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:46:51.254529800Z",
     "start_time": "2023-08-04T01:46:50.339835900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
