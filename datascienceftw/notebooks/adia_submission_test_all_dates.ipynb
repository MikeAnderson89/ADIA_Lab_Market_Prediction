{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-04T20:43:26.026782900Z",
     "start_time": "2023-08-04T20:43:22.947042500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-08-04T20:53:08.500052100Z",
     "start_time": "2023-08-04T20:53:08.480201200Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def convert_to_pairwise_train(X_train, y_train):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    n_samples = X_train.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_train[i, 2:], X_train[j, 2:]])\n",
    "            ids.append([X_train[i, :2], X_train[j, :2]])\n",
    "            labels.append(1 if y_train[i] > y_train[j] else 0)\n",
    "    return np.array(pairs).astype('float32'), np.array(labels).astype('float32'), np.array(ids)\n",
    "\n",
    "def convert_to_pairwise_test(X_test):\n",
    "    pairs = []\n",
    "    ids = []\n",
    "    n_samples = X_test.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_test[i, 2:], X_test[j, 2:]])\n",
    "            ids.append([X_test[i, :2], X_test[j, :2]])\n",
    "    return np.array(pairs).astype('float32'), np.array(ids)\n",
    "\n",
    "\n",
    "def pairwise_generator(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i] > batch_y[j] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator_ids(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            X_ids_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                        X_ids_batch.append(X_id_pair)\n",
    "\n",
    "            yield np.array(X_batch), np.array(X_ids_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator2(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]  # Pairs are now complete rows\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator3(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                    if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(y_pair)\n",
    "                        generated_pairs.add(pair_key)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        X_batch_ids = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    X_pair_ids = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    X_batch_ids.append(X_batch_ids)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32'), np.array(X_batch_ids)\n",
    "                        X_batch = []\n",
    "                        y_batch = []\n",
    "                        X_batch_ids = []\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Create a dictionary to store indices for each date\n",
    "        date_indices = defaultdict(list)\n",
    "        for i in range(n_samples):\n",
    "            date_indices[batch_X[i, 0]].append(i)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = date_indices[date_i]\n",
    "            for j in same_date_indices:\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                pair_key = (i, j)  # Use indices directly as a unique key\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = np.array([batch_X[i, 2:], batch_X[j, 2:]])\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32').reshape(-1,1)\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "### TRAIN METHODOLOGY 2 ###\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 150\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch = 1000,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = 500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization,\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "        ])\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")\n",
    "\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:53:09.778399700Z",
     "start_time": "2023-08-04T20:53:09.759895800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Load PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    # PCA\n",
    "    pca_features = pca.transform(X_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 1000\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:53:11.117520Z",
     "start_time": "2023-08-04T20:53:11.110519300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:53:13.947238500Z",
     "start_time": "2023-08-04T20:53:12.241341400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415591, 42)\n",
      "Epoch 1/30\n",
      " 986/1000 [============================>.] - ETA: 0s - loss: 2.0753 - accuracy: 0.5146\n",
      "Epoch 1: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.0606 - accuracy: 0.5124 - val_loss: 1.0052 - val_accuracy: 0.5197\n",
      "Epoch 2/30\n",
      " 996/1000 [============================>.] - ETA: 0s - loss: 0.8060 - accuracy: 0.5351\n",
      "Epoch 2: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8058 - accuracy: 0.5347 - val_loss: 0.7332 - val_accuracy: 0.4682\n",
      "Epoch 3/30\n",
      " 996/1000 [============================>.] - ETA: 0s - loss: 0.7054 - accuracy: 0.5124\n",
      "Epoch 3: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7053 - accuracy: 0.5136 - val_loss: 0.6950 - val_accuracy: 0.5242\n",
      "Epoch 4/30\n",
      " 995/1000 [============================>.] - ETA: 0s - loss: 0.6951 - accuracy: 0.4882\n",
      "Epoch 4: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6951 - accuracy: 0.4872 - val_loss: 0.6930 - val_accuracy: 0.5235\n",
      "Epoch 5/30\n",
      " 988/1000 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.5055\n",
      "Epoch 5: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6935 - accuracy: 0.5006 - val_loss: 0.6925 - val_accuracy: 0.5437\n",
      "Epoch 6/30\n",
      " 988/1000 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5287\n",
      "Epoch 6: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6928 - accuracy: 0.5277 - val_loss: 0.6938 - val_accuracy: 0.4856\n",
      "Epoch 7/30\n",
      " 989/1000 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5115\n",
      "Epoch 7: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6931 - accuracy: 0.5141 - val_loss: 0.6910 - val_accuracy: 0.5579\n",
      "Epoch 8/30\n",
      " 991/1000 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.4953\n",
      "Epoch 8: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6937 - accuracy: 0.4962 - val_loss: 0.6937 - val_accuracy: 0.4865\n",
      "Epoch 9/30\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5160\n",
      "Epoch 9: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6929 - accuracy: 0.5169 - val_loss: 0.6938 - val_accuracy: 0.4890\n",
      "Epoch 10/30\n",
      " 991/1000 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5265\n",
      "Epoch 10: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6933 - val_accuracy: 0.5038\n",
      "Epoch 11/30\n",
      " 998/1000 [============================>.] - ETA: 0s - loss: 0.6943 - accuracy: 0.4852\n",
      "Epoch 11: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6943 - accuracy: 0.4844 - val_loss: 0.6936 - val_accuracy: 0.4865\n",
      "Epoch 12/30\n",
      " 989/1000 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5192\n",
      "Epoch 12: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6929 - accuracy: 0.5172 - val_loss: 0.6933 - val_accuracy: 0.5001\n",
      "Epoch 13/30\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4650\n",
      "Epoch 13: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6939 - accuracy: 0.4654 - val_loss: 0.6932 - val_accuracy: 0.4905\n",
      "Epoch 14/30\n",
      " 989/1000 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5220\n",
      "Epoch 14: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.6934 - val_accuracy: 0.4942\n",
      "Epoch 15/30\n",
      " 993/1000 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.5229\n",
      "Epoch 15: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6927 - accuracy: 0.5241 - val_loss: 0.6957 - val_accuracy: 0.4443\n",
      "Epoch 16/30\n",
      " 986/1000 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.4847\n",
      "Epoch 16: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6940 - accuracy: 0.4869 - val_loss: 0.6935 - val_accuracy: 0.4880\n",
      "Epoch 17/30\n",
      " 983/1000 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5154\n",
      "Epoch 17: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6930 - accuracy: 0.5161 - val_loss: 0.6943 - val_accuracy: 0.4685\n",
      "Epoch 18/30\n",
      " 987/1000 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5162\n",
      "Epoch 18: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6929 - accuracy: 0.5174 - val_loss: 0.6941 - val_accuracy: 0.4822\n",
      "Epoch 19/30\n",
      " 998/1000 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5037\n",
      "Epoch 19: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6934 - accuracy: 0.5046 - val_loss: 0.6937 - val_accuracy: 0.4883\n",
      "Epoch 20/30\n",
      " 996/1000 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5171\n",
      "Epoch 20: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6930 - accuracy: 0.5157 - val_loss: 0.6926 - val_accuracy: 0.5176\n",
      "Epoch 21/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5201\n",
      "Epoch 21: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6927 - accuracy: 0.5198 - val_loss: 0.6923 - val_accuracy: 0.5236\n",
      "Epoch 22/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.4942\n",
      "Epoch 22: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6939 - accuracy: 0.4946 - val_loss: 0.6925 - val_accuracy: 0.5218\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5272\n",
      "Epoch 23: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6924 - accuracy: 0.5272 - val_loss: 0.6896 - val_accuracy: 0.5675\n",
      "Epoch 24/30\n",
      " 985/1000 [============================>.] - ETA: 0s - loss: 0.6947 - accuracy: 0.4636\n",
      "Epoch 24: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6947 - accuracy: 0.4600 - val_loss: 0.6929 - val_accuracy: 0.5457\n",
      "Epoch 25/30\n",
      " 995/1000 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4768\n",
      "Epoch 25: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6934 - accuracy: 0.4755 - val_loss: 0.6933 - val_accuracy: 0.4474\n",
      "Epoch 26/30\n",
      " 984/1000 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5218\n",
      "Epoch 26: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6930 - accuracy: 0.5231 - val_loss: 0.6941 - val_accuracy: 0.4620\n",
      "Epoch 27/30\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5169\n",
      "Epoch 27: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6930 - accuracy: 0.5164 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 28/30\n",
      " 994/1000 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5216\n",
      "Epoch 28: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6928 - accuracy: 0.5205 - val_loss: 0.6935 - val_accuracy: 0.4983\n",
      "Epoch 29/30\n",
      " 985/1000 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5155\n",
      "Epoch 29: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6928 - accuracy: 0.5160 - val_loss: 0.6945 - val_accuracy: 0.4806\n",
      "Epoch 30/30\n",
      " 988/1000 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5243\n",
      "Epoch 30: saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6924 - accuracy: 0.5248 - val_loss: 0.6938 - val_accuracy: 0.4963\n",
      "Finished All Training\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:55:41.637806400Z",
     "start_time": "2023-08-04T20:53:26.183878400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([440574,  35448,  81110, 190543, 137985, 699978, 389398, 491170,\\n            478644, 221578,\\n            ...\\n            582616,  38200, 222407,     76, 685371, 434071, 181000, 680980,\\n            326517, 278034],\\n           dtype='int64', length=742670)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m load_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../resources/model.keras\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m val_loss, val_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpairwise_generator5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\n\u001B[0;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[1;32mIn[18], line 198\u001B[0m, in \u001B[0;36mpairwise_generator5\u001B[1;34m(X, y, batch_size)\u001B[0m\n\u001B[0;32m    196\u001B[0m indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(n_samples)\n\u001B[0;32m    197\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mshuffle(indices)  \u001B[38;5;66;03m# Shuffle indices to create random batches\u001B[39;00m\n\u001B[1;32m--> 198\u001B[0m batch_X \u001B[38;5;241m=\u001B[39m \u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    199\u001B[0m batch_y \u001B[38;5;241m=\u001B[39m y[indices]\n\u001B[0;32m    201\u001B[0m X_batch \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3811\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   3812\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 3813\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3815\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6067\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6068\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6070\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6072\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6074\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_interval_msg:\n\u001B[0;32m   6129\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 6130\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6132\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m   6133\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Int64Index([440574,  35448,  81110, 190543, 137985, 699978, 389398, 491170,\\n            478644, 221578,\\n            ...\\n            582616,  38200, 222407,     76, 685371, 434071, 181000, 680980,\\n            326517, 278034],\\n           dtype='int64', length=742670)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "model = load_model('../resources/model.keras')\n",
    "val_loss, val_accuracy = model.evaluate(\n",
    "    pairwise_generator5(X_train, y_train, 500),\n",
    "    steps=1000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:57:26.923075200Z",
     "start_time": "2023-08-04T20:57:24.251377600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200].iloc[:100]\n",
    "y_test = y_train[y_train['date'] == 200].iloc[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:57:29.470861900Z",
     "start_time": "2023-08-04T20:57:29.447255900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mX_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mzscore\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   9557\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   9559\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   9560\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   9561\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   9566\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   9567\u001B[0m )\n\u001B[1;32m-> 9568\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:764\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:891\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 891\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    893\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:907\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    905\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    906\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 907\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    908\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    909\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    911\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:2713\u001B[0m, in \u001B[0;36mzscore\u001B[1;34m(a, axis, ddof, nan_policy)\u001B[0m\n\u001B[0;32m   2644\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mzscore\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ddof\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, nan_policy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpropagate\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m   2645\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2646\u001B[0m \u001B[38;5;124;03m    Compute the z score.\u001B[39;00m\n\u001B[0;32m   2647\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2711\u001B[0m \u001B[38;5;124;03m           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\u001B[39;00m\n\u001B[0;32m   2712\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2713\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mzmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mddof\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mddof\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnan_policy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnan_policy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:2872\u001B[0m, in \u001B[0;36mzmap\u001B[1;34m(scores, compare, axis, ddof, nan_policy)\u001B[0m\n\u001B[0;32m   2870\u001B[0m         isconst \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mapply_along_axis(_isconst, axis, a)\n\u001B[0;32m   2871\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2872\u001B[0m     mn \u001B[38;5;241m=\u001B[39m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2873\u001B[0m     std \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mstd(axis\u001B[38;5;241m=\u001B[39maxis, ddof\u001B[38;5;241m=\u001B[39mddof, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   2874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:182\u001B[0m, in \u001B[0;36m_mean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m    180\u001B[0m ret \u001B[38;5;241m=\u001B[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001B[38;5;241m=\u001B[39mwhere)\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 182\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mum\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrue_divide\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m            \u001B[49m\u001B[43mret\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrcount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mret\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43munsafe\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_float16_result \u001B[38;5;129;01mand\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    185\u001B[0m         ret \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mtype(ret)\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "X_train[X_train['date'] == 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T19:55:52.432944Z",
     "start_time": "2023-08-04T19:55:50.848302500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for Test Data\n",
      "155/155 [==============================] - 0s 2ms/step\n",
      "Finished predicting Test Data\n"
     ]
    }
   ],
   "source": [
    "test = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:57:42.700432900Z",
     "start_time": "2023-08-04T20:57:42.064756700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "    date                                                 id         0  \\\n0    200  b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de... -0.395584   \n1    200  77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...  1.560541   \n2    200  f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1... -0.004308   \n3    200  15e3163819739dc4b9318670fbda0c7361b25dc31b3025...  1.032348   \n4    200  b61439cc04e2a6666b7b3834124952b00742885c74b4a3...  0.764759   \n..   ...                                                ...       ...   \n95   200  5f6c6e9cc1e1aba9aab4fbd4489c1978d5cf281c971b26...  0.946306   \n96   200  1731cf53cff346ed7e47430dee7a317294db7b7387d5d4...  0.940856   \n97   200  0c8977414e7c002bd97bc1ac870155bedf845a291e096b... -0.909378   \n98   200  002a1309181f9e7e2eb0557c4dca257d4197be58981b2a... -0.812412   \n99   200  d92058bddbbef37276ebdfe0ee0d384476effa58e72a30...  0.311698   \n\n           1         2         3         4         5         6         7  ...  \\\n0   0.467767  0.334684 -0.003670 -0.084305 -0.610649 -1.911305  1.869958  ...   \n1  -0.345553  0.395517  0.538143  0.061281 -0.426155 -0.099393  0.442215  ...   \n2   1.535068 -0.351770 -0.574043 -0.736015 -0.483100  1.136352  0.610554  ...   \n3  -0.108589  0.112654 -0.984792  0.000401 -0.497814  0.447101 -0.370030  ...   \n4  -0.020000 -0.174101  0.250532 -0.470467  1.320027  0.844867 -0.067772  ...   \n..       ...       ...       ...       ...       ...       ...       ...  ...   \n95  0.174979  0.472097 -0.425053  0.203503  2.093819  0.157956  0.368474  ...   \n96  0.458419  0.573755  0.127610 -0.120156  0.349935  1.293528  0.145554  ...   \n97 -0.389497 -1.426744  0.497973 -0.383100  1.234804  0.322476  0.101190  ...   \n98 -0.683046 -1.333758  0.900040  0.318777  1.192798  0.447456  0.103943  ...   \n99 -0.751044  0.026300  1.780147 -1.418370 -0.122515  1.499807  0.844081  ...   \n\n         452       453       454       455       456       457       458  \\\n0   0.064613  0.063679 -0.942424  0.437068  0.162571 -0.511908  0.700700   \n1  -1.091513 -0.949481  0.495675  0.373371 -0.370041  1.043447 -0.436087   \n2   0.613645 -0.076734  0.458068 -1.385604 -1.153192  2.216623  1.369278   \n3  -0.917909  0.796511 -1.143900 -0.840966 -1.124376 -0.075706  0.875465   \n4  -1.779329 -0.287168  1.196917  0.474941  0.009121 -0.285514 -1.211052   \n..       ...       ...       ...       ...       ...       ...       ...   \n95 -0.056957 -0.243457 -0.240920  0.580476 -0.585203  0.696882 -0.258386   \n96  0.064852  0.439111  1.256536 -1.425672 -0.791057  0.558181  0.024659   \n97 -1.074217 -0.510769 -1.025943  0.338949  0.143359 -1.936703  0.057343   \n98 -1.128484 -0.906456  1.199849 -0.729728  0.334035  0.411012  0.341870   \n99  0.272101 -0.547933 -0.720405 -1.936129  1.677504  0.620226 -0.582131   \n\n         459       460  value  \n0   0.531277  0.554467   -1.0  \n1  -1.206550  0.870037   -1.0  \n2  -0.214589  1.349027   -1.0  \n3  -0.645114  0.108495   -1.0  \n4   3.517855 -1.681823   -1.0  \n..       ...       ...    ...  \n95  1.064468  0.977430   -1.0  \n96 -0.445847  1.158122   -1.0  \n97 -0.419597 -0.998607   -1.0  \n98  0.655641  0.402195   -1.0  \n99 -0.677034 -0.231281   -1.0  \n\n[100 rows x 464 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>200</td>\n      <td>b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de...</td>\n      <td>-0.395584</td>\n      <td>0.467767</td>\n      <td>0.334684</td>\n      <td>-0.003670</td>\n      <td>-0.084305</td>\n      <td>-0.610649</td>\n      <td>-1.911305</td>\n      <td>1.869958</td>\n      <td>...</td>\n      <td>0.064613</td>\n      <td>0.063679</td>\n      <td>-0.942424</td>\n      <td>0.437068</td>\n      <td>0.162571</td>\n      <td>-0.511908</td>\n      <td>0.700700</td>\n      <td>0.531277</td>\n      <td>0.554467</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...</td>\n      <td>1.560541</td>\n      <td>-0.345553</td>\n      <td>0.395517</td>\n      <td>0.538143</td>\n      <td>0.061281</td>\n      <td>-0.426155</td>\n      <td>-0.099393</td>\n      <td>0.442215</td>\n      <td>...</td>\n      <td>-1.091513</td>\n      <td>-0.949481</td>\n      <td>0.495675</td>\n      <td>0.373371</td>\n      <td>-0.370041</td>\n      <td>1.043447</td>\n      <td>-0.436087</td>\n      <td>-1.206550</td>\n      <td>0.870037</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>200</td>\n      <td>f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1...</td>\n      <td>-0.004308</td>\n      <td>1.535068</td>\n      <td>-0.351770</td>\n      <td>-0.574043</td>\n      <td>-0.736015</td>\n      <td>-0.483100</td>\n      <td>1.136352</td>\n      <td>0.610554</td>\n      <td>...</td>\n      <td>0.613645</td>\n      <td>-0.076734</td>\n      <td>0.458068</td>\n      <td>-1.385604</td>\n      <td>-1.153192</td>\n      <td>2.216623</td>\n      <td>1.369278</td>\n      <td>-0.214589</td>\n      <td>1.349027</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>200</td>\n      <td>15e3163819739dc4b9318670fbda0c7361b25dc31b3025...</td>\n      <td>1.032348</td>\n      <td>-0.108589</td>\n      <td>0.112654</td>\n      <td>-0.984792</td>\n      <td>0.000401</td>\n      <td>-0.497814</td>\n      <td>0.447101</td>\n      <td>-0.370030</td>\n      <td>...</td>\n      <td>-0.917909</td>\n      <td>0.796511</td>\n      <td>-1.143900</td>\n      <td>-0.840966</td>\n      <td>-1.124376</td>\n      <td>-0.075706</td>\n      <td>0.875465</td>\n      <td>-0.645114</td>\n      <td>0.108495</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>b61439cc04e2a6666b7b3834124952b00742885c74b4a3...</td>\n      <td>0.764759</td>\n      <td>-0.020000</td>\n      <td>-0.174101</td>\n      <td>0.250532</td>\n      <td>-0.470467</td>\n      <td>1.320027</td>\n      <td>0.844867</td>\n      <td>-0.067772</td>\n      <td>...</td>\n      <td>-1.779329</td>\n      <td>-0.287168</td>\n      <td>1.196917</td>\n      <td>0.474941</td>\n      <td>0.009121</td>\n      <td>-0.285514</td>\n      <td>-1.211052</td>\n      <td>3.517855</td>\n      <td>-1.681823</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>200</td>\n      <td>5f6c6e9cc1e1aba9aab4fbd4489c1978d5cf281c971b26...</td>\n      <td>0.946306</td>\n      <td>0.174979</td>\n      <td>0.472097</td>\n      <td>-0.425053</td>\n      <td>0.203503</td>\n      <td>2.093819</td>\n      <td>0.157956</td>\n      <td>0.368474</td>\n      <td>...</td>\n      <td>-0.056957</td>\n      <td>-0.243457</td>\n      <td>-0.240920</td>\n      <td>0.580476</td>\n      <td>-0.585203</td>\n      <td>0.696882</td>\n      <td>-0.258386</td>\n      <td>1.064468</td>\n      <td>0.977430</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>200</td>\n      <td>1731cf53cff346ed7e47430dee7a317294db7b7387d5d4...</td>\n      <td>0.940856</td>\n      <td>0.458419</td>\n      <td>0.573755</td>\n      <td>0.127610</td>\n      <td>-0.120156</td>\n      <td>0.349935</td>\n      <td>1.293528</td>\n      <td>0.145554</td>\n      <td>...</td>\n      <td>0.064852</td>\n      <td>0.439111</td>\n      <td>1.256536</td>\n      <td>-1.425672</td>\n      <td>-0.791057</td>\n      <td>0.558181</td>\n      <td>0.024659</td>\n      <td>-0.445847</td>\n      <td>1.158122</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>200</td>\n      <td>0c8977414e7c002bd97bc1ac870155bedf845a291e096b...</td>\n      <td>-0.909378</td>\n      <td>-0.389497</td>\n      <td>-1.426744</td>\n      <td>0.497973</td>\n      <td>-0.383100</td>\n      <td>1.234804</td>\n      <td>0.322476</td>\n      <td>0.101190</td>\n      <td>...</td>\n      <td>-1.074217</td>\n      <td>-0.510769</td>\n      <td>-1.025943</td>\n      <td>0.338949</td>\n      <td>0.143359</td>\n      <td>-1.936703</td>\n      <td>0.057343</td>\n      <td>-0.419597</td>\n      <td>-0.998607</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>200</td>\n      <td>002a1309181f9e7e2eb0557c4dca257d4197be58981b2a...</td>\n      <td>-0.812412</td>\n      <td>-0.683046</td>\n      <td>-1.333758</td>\n      <td>0.900040</td>\n      <td>0.318777</td>\n      <td>1.192798</td>\n      <td>0.447456</td>\n      <td>0.103943</td>\n      <td>...</td>\n      <td>-1.128484</td>\n      <td>-0.906456</td>\n      <td>1.199849</td>\n      <td>-0.729728</td>\n      <td>0.334035</td>\n      <td>0.411012</td>\n      <td>0.341870</td>\n      <td>0.655641</td>\n      <td>0.402195</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>200</td>\n      <td>d92058bddbbef37276ebdfe0ee0d384476effa58e72a30...</td>\n      <td>0.311698</td>\n      <td>-0.751044</td>\n      <td>0.026300</td>\n      <td>1.780147</td>\n      <td>-1.418370</td>\n      <td>-0.122515</td>\n      <td>1.499807</td>\n      <td>0.844081</td>\n      <td>...</td>\n      <td>0.272101</td>\n      <td>-0.547933</td>\n      <td>-0.720405</td>\n      <td>-1.936129</td>\n      <td>1.677504</td>\n      <td>0.620226</td>\n      <td>-0.582131</td>\n      <td>-0.677034</td>\n      <td>-0.231281</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 464 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:57:45.774542900Z",
     "start_time": "2023-08-04T20:57:45.754542800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.66758526, -0.81911179, -0.07204253, -0.02755289],\n       [ 1.05854155, -0.69364701, -0.12870892,  1.82694012],\n       [ 0.70995108, -0.42835449,  1.09309793, -0.22740316],\n       [ 0.5274588 ,  1.91407711,  0.854834  , -0.3534117 ],\n       [-1.62836617,  0.02703618, -1.74718048, -1.21857237]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:10:17.920222700Z",
     "start_time": "2023-08-04T20:10:17.898223300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:51:52.648208200Z",
     "start_time": "2023-08-04T20:51:50.837497100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415591, 42)\n"
     ]
    }
   ],
   "source": [
    "max_date = X_train['date'].max()\n",
    "model_directory_path = '../resources'\n",
    "min_date = 150\n",
    "X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "X_test = X_train[X_train['date'] == max_date]\n",
    "y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "#PCA\n",
    "n_components = 40\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_features = pca.fit_transform(X_scale_pca)\n",
    "X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "pca_features_test = pca.transform(X_test_scale_pca)\n",
    "X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "#Save out Scaler and PCA\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "batch_size = 1500\n",
    "train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "# #Model Training\n",
    "# model_pathname = Path('../resources') / \"model.keras\"\n",
    "#\n",
    "# if model_pathname.is_file():\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss',\n",
    "#         patience=20,\n",
    "#         verbose=0,\n",
    "#         mode='auto',\n",
    "#         baseline=None)\n",
    "#\n",
    "#     history = model.fit(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch = len(X_train_concat) // batch_size,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=30,\n",
    "#         validation_data=test_generator,\n",
    "#         validation_steps = len(X_test_concat) // batch_size,\n",
    "#         callbacks=[mc, early_stopping],\n",
    "#         shuffle=True,\n",
    "#         use_multiprocessing=False,\n",
    "#         verbose=1\n",
    "#     )\n",
    "#\n",
    "# else:\n",
    "#     #Neural Network Model\n",
    "#     mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "#\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss',\n",
    "#         patience=20,\n",
    "#         verbose=1,\n",
    "#         mode='auto',\n",
    "#         baseline=None)\n",
    "#\n",
    "#     model = keras.Sequential([\n",
    "#         keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "#         #keras.layers.BatchNormalization(),\n",
    "#         keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "#         keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "#         #keras.layers.BatchNormalization(),\n",
    "#         # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "#         # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "#         keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "#         keras.layers.BatchNormalization(),\n",
    "#         keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "#         keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "#         keras.layers.Dropout(0.5),  # Adding dropout regularization,\n",
    "#         keras.layers.Flatten(),\n",
    "#         keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "#     ])\n",
    "#     model.summary()\n",
    "#     optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "#\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#\n",
    "#     history = model.fit(\n",
    "#         train_generator,\n",
    "#         batch_size=batch_size,\n",
    "#         steps_per_epoch=len(X_train_concat) // batch_size,\n",
    "#         epochs=30,\n",
    "#         validation_steps=len(X_test_concat) // batch_size,\n",
    "#         validation_data=test_generator,\n",
    "#         callbacks=[mc, early_stopping],\n",
    "#         shuffle=True,\n",
    "#         use_multiprocessing=False,\n",
    "#         verbose=1\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:51:58.960550400Z",
     "start_time": "2023-08-04T20:51:53.523555700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(1500, 1)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pairs, y_pairs = next(train_generator)\n",
    "y_pairs.reshape(-1,1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:52:32.928796400Z",
     "start_time": "2023-08-04T20:52:32.913795700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pairs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:06:10.948924300Z",
     "start_time": "2023-08-04T01:06:10.942924600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:27:16.394465300Z",
     "start_time": "2023-08-04T01:27:16.380309300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "test_x, test_y = next(train_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:43:07.148504600Z",
     "start_time": "2023-08-03T14:43:07.131005400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200]\n",
    "y_test = y_train[y_train['date'] == 200]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:43:44.709178400Z",
     "start_time": "2023-08-04T20:43:44.674180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "        date                                                 id         0  \\\n489422   200  b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de... -0.395584   \n489423   200  77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...  1.560541   \n489424   200  f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1... -0.004308   \n489425   200  15e3163819739dc4b9318670fbda0c7361b25dc31b3025...  1.032348   \n489426   200  b61439cc04e2a6666b7b3834124952b00742885c74b4a3...  0.764759   \n...      ...                                                ...       ...   \n492839   200  ebcc110654e89b43d198347ffdf95155a29dd820cba947...  0.745428   \n492840   200  ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb... -0.961760   \n492841   200  92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad... -2.151686   \n492842   200  2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a... -1.108856   \n492843   200  054ba9c0ace464311f89cbda16873a29566ff120e9c8bd... -0.878197   \n\n               1         2         3         4         5         6         7  \\\n489422  0.467767  0.334684 -0.003670 -0.084305 -0.610649 -1.911305  1.869958   \n489423 -0.345553  0.395517  0.538143  0.061281 -0.426155 -0.099393  0.442215   \n489424  1.535068 -0.351770 -0.574043 -0.736015 -0.483100  1.136352  0.610554   \n489425 -0.108589  0.112654 -0.984792  0.000401 -0.497814  0.447101 -0.370030   \n489426 -0.020000 -0.174101  0.250532 -0.470467  1.320027  0.844867 -0.067772   \n...          ...       ...       ...       ...       ...       ...       ...   \n492839  0.440497 -0.381767  1.300897  0.678993 -1.070824 -0.163306  0.051261   \n492840 -0.365210 -2.539697  2.138631  0.538042 -0.479963  0.536242 -0.829076   \n492841  2.589724  0.579317 -0.722497 -0.456387 -0.235593 -1.049381  0.218806   \n492842  0.340626  1.462280  0.606334 -0.547911 -0.259995  0.471951  1.366694   \n492843 -0.467488  1.553955  1.128971  1.562995 -1.405781 -1.011634 -1.937196   \n\n        ...       451       452       453       454       455       456  \\\n489422  ...  0.072215  0.064613  0.063679 -0.942424  0.437068  0.162571   \n489423  ...  2.041005 -1.091513 -0.949481  0.495675  0.373371 -0.370041   \n489424  ... -0.076245  0.613645 -0.076734  0.458068 -1.385604 -1.153192   \n489425  ...  0.302309 -0.917909  0.796511 -1.143900 -0.840966 -1.124376   \n489426  ...  0.526456 -1.779329 -0.287168  1.196917  0.474941  0.009121   \n...     ...       ...       ...       ...       ...       ...       ...   \n492839  ... -0.479455 -1.193991  0.057748 -0.266845 -1.978869  0.173025   \n492840  ... -0.181034 -0.642921  1.288435 -0.998922 -0.983312  0.108591   \n492841  ... -0.037265  1.733668  0.443702  0.438292  0.559468 -1.700752   \n492842  ...  0.862505  0.735640 -1.822171 -0.510724 -2.559841 -0.556591   \n492843  ... -0.821479 -0.165956 -0.031798  0.334694 -1.616378 -0.463422   \n\n             457       458       459       460  \n489422 -0.511908  0.700700  0.531277  0.554467  \n489423  1.043447 -0.436087 -1.206550  0.870037  \n489424  2.216623  1.369278 -0.214589  1.349027  \n489425 -0.075706  0.875465 -0.645114  0.108495  \n489426 -0.285514 -1.211052  3.517855 -1.681823  \n...          ...       ...       ...       ...  \n492839  0.636986 -0.145487 -1.612342  0.670242  \n492840 -0.109811  0.109938 -1.127231  0.548930  \n492841 -2.230316 -0.355390  0.310145 -1.594635  \n492842  1.892561  1.818054 -0.408390  0.190328  \n492843  1.549226  0.209962  0.020122  0.301778  \n\n[3422 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>489422</th>\n      <td>200</td>\n      <td>b2014a4eb6f5be0b43de60668d6cda68f481fd1a3554de...</td>\n      <td>-0.395584</td>\n      <td>0.467767</td>\n      <td>0.334684</td>\n      <td>-0.003670</td>\n      <td>-0.084305</td>\n      <td>-0.610649</td>\n      <td>-1.911305</td>\n      <td>1.869958</td>\n      <td>...</td>\n      <td>0.072215</td>\n      <td>0.064613</td>\n      <td>0.063679</td>\n      <td>-0.942424</td>\n      <td>0.437068</td>\n      <td>0.162571</td>\n      <td>-0.511908</td>\n      <td>0.700700</td>\n      <td>0.531277</td>\n      <td>0.554467</td>\n    </tr>\n    <tr>\n      <th>489423</th>\n      <td>200</td>\n      <td>77a12377576612a9a90570b270df5e6fe86f20fd8bacfb...</td>\n      <td>1.560541</td>\n      <td>-0.345553</td>\n      <td>0.395517</td>\n      <td>0.538143</td>\n      <td>0.061281</td>\n      <td>-0.426155</td>\n      <td>-0.099393</td>\n      <td>0.442215</td>\n      <td>...</td>\n      <td>2.041005</td>\n      <td>-1.091513</td>\n      <td>-0.949481</td>\n      <td>0.495675</td>\n      <td>0.373371</td>\n      <td>-0.370041</td>\n      <td>1.043447</td>\n      <td>-0.436087</td>\n      <td>-1.206550</td>\n      <td>0.870037</td>\n    </tr>\n    <tr>\n      <th>489424</th>\n      <td>200</td>\n      <td>f0cce9977234a16b9171182664a0ef16f2fa373eb4b8c1...</td>\n      <td>-0.004308</td>\n      <td>1.535068</td>\n      <td>-0.351770</td>\n      <td>-0.574043</td>\n      <td>-0.736015</td>\n      <td>-0.483100</td>\n      <td>1.136352</td>\n      <td>0.610554</td>\n      <td>...</td>\n      <td>-0.076245</td>\n      <td>0.613645</td>\n      <td>-0.076734</td>\n      <td>0.458068</td>\n      <td>-1.385604</td>\n      <td>-1.153192</td>\n      <td>2.216623</td>\n      <td>1.369278</td>\n      <td>-0.214589</td>\n      <td>1.349027</td>\n    </tr>\n    <tr>\n      <th>489425</th>\n      <td>200</td>\n      <td>15e3163819739dc4b9318670fbda0c7361b25dc31b3025...</td>\n      <td>1.032348</td>\n      <td>-0.108589</td>\n      <td>0.112654</td>\n      <td>-0.984792</td>\n      <td>0.000401</td>\n      <td>-0.497814</td>\n      <td>0.447101</td>\n      <td>-0.370030</td>\n      <td>...</td>\n      <td>0.302309</td>\n      <td>-0.917909</td>\n      <td>0.796511</td>\n      <td>-1.143900</td>\n      <td>-0.840966</td>\n      <td>-1.124376</td>\n      <td>-0.075706</td>\n      <td>0.875465</td>\n      <td>-0.645114</td>\n      <td>0.108495</td>\n    </tr>\n    <tr>\n      <th>489426</th>\n      <td>200</td>\n      <td>b61439cc04e2a6666b7b3834124952b00742885c74b4a3...</td>\n      <td>0.764759</td>\n      <td>-0.020000</td>\n      <td>-0.174101</td>\n      <td>0.250532</td>\n      <td>-0.470467</td>\n      <td>1.320027</td>\n      <td>0.844867</td>\n      <td>-0.067772</td>\n      <td>...</td>\n      <td>0.526456</td>\n      <td>-1.779329</td>\n      <td>-0.287168</td>\n      <td>1.196917</td>\n      <td>0.474941</td>\n      <td>0.009121</td>\n      <td>-0.285514</td>\n      <td>-1.211052</td>\n      <td>3.517855</td>\n      <td>-1.681823</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>492839</th>\n      <td>200</td>\n      <td>ebcc110654e89b43d198347ffdf95155a29dd820cba947...</td>\n      <td>0.745428</td>\n      <td>0.440497</td>\n      <td>-0.381767</td>\n      <td>1.300897</td>\n      <td>0.678993</td>\n      <td>-1.070824</td>\n      <td>-0.163306</td>\n      <td>0.051261</td>\n      <td>...</td>\n      <td>-0.479455</td>\n      <td>-1.193991</td>\n      <td>0.057748</td>\n      <td>-0.266845</td>\n      <td>-1.978869</td>\n      <td>0.173025</td>\n      <td>0.636986</td>\n      <td>-0.145487</td>\n      <td>-1.612342</td>\n      <td>0.670242</td>\n    </tr>\n    <tr>\n      <th>492840</th>\n      <td>200</td>\n      <td>ed93af2fb3db2b9150cd7896b2aed25973867f7b2ed9eb...</td>\n      <td>-0.961760</td>\n      <td>-0.365210</td>\n      <td>-2.539697</td>\n      <td>2.138631</td>\n      <td>0.538042</td>\n      <td>-0.479963</td>\n      <td>0.536242</td>\n      <td>-0.829076</td>\n      <td>...</td>\n      <td>-0.181034</td>\n      <td>-0.642921</td>\n      <td>1.288435</td>\n      <td>-0.998922</td>\n      <td>-0.983312</td>\n      <td>0.108591</td>\n      <td>-0.109811</td>\n      <td>0.109938</td>\n      <td>-1.127231</td>\n      <td>0.548930</td>\n    </tr>\n    <tr>\n      <th>492841</th>\n      <td>200</td>\n      <td>92d48ab6987d778ddc8b2eca74ff6a5c52d13afbf0e1ad...</td>\n      <td>-2.151686</td>\n      <td>2.589724</td>\n      <td>0.579317</td>\n      <td>-0.722497</td>\n      <td>-0.456387</td>\n      <td>-0.235593</td>\n      <td>-1.049381</td>\n      <td>0.218806</td>\n      <td>...</td>\n      <td>-0.037265</td>\n      <td>1.733668</td>\n      <td>0.443702</td>\n      <td>0.438292</td>\n      <td>0.559468</td>\n      <td>-1.700752</td>\n      <td>-2.230316</td>\n      <td>-0.355390</td>\n      <td>0.310145</td>\n      <td>-1.594635</td>\n    </tr>\n    <tr>\n      <th>492842</th>\n      <td>200</td>\n      <td>2cc2ea6e7ee8eaabe679620c414f07840c5e9226f28d0a...</td>\n      <td>-1.108856</td>\n      <td>0.340626</td>\n      <td>1.462280</td>\n      <td>0.606334</td>\n      <td>-0.547911</td>\n      <td>-0.259995</td>\n      <td>0.471951</td>\n      <td>1.366694</td>\n      <td>...</td>\n      <td>0.862505</td>\n      <td>0.735640</td>\n      <td>-1.822171</td>\n      <td>-0.510724</td>\n      <td>-2.559841</td>\n      <td>-0.556591</td>\n      <td>1.892561</td>\n      <td>1.818054</td>\n      <td>-0.408390</td>\n      <td>0.190328</td>\n    </tr>\n    <tr>\n      <th>492843</th>\n      <td>200</td>\n      <td>054ba9c0ace464311f89cbda16873a29566ff120e9c8bd...</td>\n      <td>-0.878197</td>\n      <td>-0.467488</td>\n      <td>1.553955</td>\n      <td>1.128971</td>\n      <td>1.562995</td>\n      <td>-1.405781</td>\n      <td>-1.011634</td>\n      <td>-1.937196</td>\n      <td>...</td>\n      <td>-0.821479</td>\n      <td>-0.165956</td>\n      <td>-0.031798</td>\n      <td>0.334694</td>\n      <td>-1.616378</td>\n      <td>-0.463422</td>\n      <td>1.549226</td>\n      <td>0.209962</td>\n      <td>0.020122</td>\n      <td>0.301778</td>\n    </tr>\n  </tbody>\n</table>\n<p>3422 rows × 463 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:43:45.598681400Z",
     "start_time": "2023-08-04T20:43:45.566931400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3422, 42)\n",
      "(5853331, 2, 40)\n",
      "(5853331, 2, 2)\n",
      "Predicting for Test Data\n",
      "182917/182917 [==============================] - 304s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test_orig = X_test.copy()\n",
    "model_directory_path = '../resources'\n",
    "\n",
    "# Load Scaler\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Load PCA\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "# Scaling\n",
    "X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "# PCA\n",
    "pca_features = pca.transform(X_scale_pca)\n",
    "X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "# Load Model\n",
    "model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "model = load_model(model_pathname)\n",
    "\n",
    "# Pairwise Transformation using the pairwise generator\n",
    "batch_size = 1000\n",
    "print(X_test_concat.shape)\n",
    "X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test_ids.shape)\n",
    "\n",
    "print(\"Predicting for Test Data\")\n",
    "preds = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:50:29.281989700Z",
     "start_time": "2023-08-04T20:43:51.556838500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4800873],\n       [0.4800873],\n       [0.4800873],\n       ...,\n       [0.4800873],\n       [0.4800873],\n       [0.4800873]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:50:42.857373500Z",
     "start_time": "2023-08-04T20:50:42.831373600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T01:46:46.999054900Z",
     "start_time": "2023-08-04T01:46:46.894826300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "num_cols = [x for x in X_train.columns if x not in ['date', 'id']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:11:45.036620900Z",
     "start_time": "2023-08-04T20:11:45.026121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "X_train = X_train[X_train['date'] == 0]\n",
    "X_train[num_cols] = X_train[num_cols].apply(zscore)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:12:54.343009700Z",
     "start_time": "2023-08-04T20:12:53.488185700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_dates = list(set(X_train['date']))\n",
    "\n",
    "for date in list_of_dates:#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def convert_to_pairwise_train(X_train, y_train):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    n_samples = X_train.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_train[i, 2:], X_train[j, 2:]])\n",
    "            ids.append([X_train[i, :2], X_train[j, :2]])\n",
    "            labels.append(1 if y_train[i] > y_train[j] else 0)\n",
    "    return np.array(pairs).astype('float32'), np.array(labels).astype('float32'), np.array(ids)\n",
    "\n",
    "def convert_to_pairwise_test(X_test):\n",
    "    pairs = []\n",
    "    ids = []\n",
    "    n_samples = X_test.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_test[i, 2:], X_test[j, 2:]])\n",
    "            ids.append([X_test[i, :2], X_test[j, :2]])\n",
    "    return np.array(pairs).astype('float32'), np.array(ids)\n",
    "\n",
    "\n",
    "def pairwise_generator(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i] > batch_y[j] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator_ids(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            batch_y = y[start:end]\n",
    "\n",
    "            X_batch = []\n",
    "            X_ids_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                for j in range(i+1, len(batch_X)):\n",
    "                    date_j = batch_X[j, 0]\n",
    "                    if date_i == date_j:\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "                        X_id_pair = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                        X_ids_batch.append(X_id_pair)\n",
    "\n",
    "            yield np.array(X_batch), np.array(X_ids_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def pairwise_generator2(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]  # Pairs are now complete rows\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(1 if batch_y[i, 2] > batch_y[j, 2] else 0)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator3(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(len(batch_X)):\n",
    "                date_i = batch_X[i, 0]\n",
    "                same_date_indices = [j for j in range(i+1, len(batch_X)) if batch_X[j, 0] == date_i]\n",
    "                for j in same_date_indices:\n",
    "                    pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                    if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                        X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                        y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                        X_batch.append(X_pair)\n",
    "                        y_batch.append(y_pair)\n",
    "                        generated_pairs.add(pair_key)\n",
    "\n",
    "            yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "\n",
    "\n",
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        X_batch_ids = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    X_pair_ids = [batch_X[i, :2], batch_X[j, :2]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    X_batch_ids.append(X_batch_ids)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32'), np.array(X_batch_ids)\n",
    "                        X_batch = []\n",
    "                        y_batch = []\n",
    "                        X_batch_ids = []\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Create a dictionary to store indices for each date\n",
    "        date_indices = defaultdict(list)\n",
    "        for i in range(n_samples):\n",
    "            date_indices[batch_X[i, 0]].append(i)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = date_indices[date_i]\n",
    "            for j in same_date_indices:\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                pair_key = (i, j)  # Use indices directly as a unique key\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = np.array([batch_X[i, 2:], batch_X[j, 2:]])\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### TRAIN METHODOLOGY 2 ###\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    num_cols = [x for x in X_train.columns if x not in ['date', 'id']]\n",
    "    list_of_dates = list(set(X_train['date']))\n",
    "\n",
    "    for date in list_of_dates:\n",
    "\n",
    "\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 150\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch = len(X_train_concat) // batch_size,\n",
    "            batch_size=batch_size,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = len(X_test_concat) // batch_size,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "        ])\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=len(X_train_concat) // batch_size,\n",
    "            epochs=30,\n",
    "            validation_steps=len(X_test_concat) // batch_size,\n",
    "            validation_data=test_generator,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")\n",
    "\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Load PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    # PCA\n",
    "    pca_features = pca.transform(X_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 1000\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:19:32.836914600Z",
     "start_time": "2023-08-04T20:19:31.571435700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200]\n",
    "y_test = y_train[y_train['date'] == 200]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[X_train['date'] == 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n",
    "              [ 0.7149,  0.0775,  0.6072,  0.9656],\n",
    "              [ 0.6341,  0.1403,  0.9759,  0.4064],\n",
    "              [ 0.5918,  0.6948,  0.904 ,  0.3721],\n",
    "              [ 0.0921,  0.2481,  0.1188,  0.1366]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:16:28.470637900Z",
     "start_time": "2023-08-04T20:16:26.974301300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_date = X_train['date'].max()\n",
    "model_directory_path = '../resources'\n",
    "min_date = 150\n",
    "X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "X_test = X_train[X_train['date'] == max_date]\n",
    "y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "#PCA\n",
    "n_components = 40\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_features = pca.fit_transform(X_scale_pca)\n",
    "X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "pca_features_test = pca.transform(X_test_scale_pca)\n",
    "X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "#Save out Scaler and PCA\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "batch_size = 1500\n",
    "train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "#Model Training\n",
    "model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "if model_pathname.is_file():\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch = len(X_train_concat) // batch_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=30,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps = len(X_test_concat) // batch_size,\n",
    "        callbacks=[mc, early_stopping],\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "else:\n",
    "    #Neural Network Model\n",
    "    mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "        # keras.layers.BatchNormalization(),\n",
    "        # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "        keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),  # Adding dropout regularization,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "    ])\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=len(X_train_concat) // batch_size,\n",
    "        epochs=30,\n",
    "        validation_steps=len(X_test_concat) // batch_size,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[mc, early_stopping],\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=False,\n",
    "        verbose=1\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pairs, y_pairs = next(train_generator)\n",
    "X_pairs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pairs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pairwise_generator4(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = [j for j in range(i+1, n_samples) if batch_X[j, 0] == date_i]\n",
    "            for j in same_date_indices:\n",
    "                pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = [batch_X[i, 2:], batch_X[j, 2:]]\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32')\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_x, test_y = next(train_generator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = X_train[X_train['date'] == 200]\n",
    "y_test = y_train[y_train['date'] == 200]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_orig = X_test.copy()\n",
    "model_directory_path = '../resources'\n",
    "\n",
    "# Load Scaler\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Load PCA\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "# Scaling\n",
    "X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "# PCA\n",
    "pca_features = pca.transform(X_scale_pca)\n",
    "X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "# Load Model\n",
    "model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "model = load_model(model_pathname)\n",
    "\n",
    "# Pairwise Transformation using the pairwise generator\n",
    "batch_size = 1000\n",
    "print(X_test_concat.shape)\n",
    "X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test_ids.shape)\n",
    "\n",
    "print(\"Predicting for Test Data\")\n",
    "preds = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cols = [x for x in X_train.columns if x not in ['date', 'id']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = X_train[X_train['date'] == 0]\n",
    "X_train[num_cols] = X_train[num_cols].apply(zscore)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "list_of_dates = list(set(X_train['date']))\n",
    "\n",
    "for date in list_of_dates:\n",
    "    X_train.loc[X_train['date'] == date, num_cols] = zscore(X_train[X_train['date'] == date][num_cols])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:16:35.820046300Z",
     "start_time": "2023-08-04T20:16:35.786036900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "X_train.loc[X_train['date'] == 0, num_cols] = zscore(X_train[X_train['date'] == 0][num_cols])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:20:05.254542600Z",
     "start_time": "2023-08-04T20:20:05.036776700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6  \\\n0   -0.909515  0.388808 -1.535913 -0.133312 -1.826404 -0.532795  0.351273   \n1   -0.107694 -0.097967 -0.539599 -0.331276 -0.942609 -0.054123 -1.212772   \n2    0.092316  0.052596 -0.652025  1.218241  0.382968 -0.861838 -0.318937   \n3    4.119639  1.018918  3.687519  1.597563  0.055918 -1.406041  0.652994   \n4    0.109644 -0.290280 -0.278987 -0.603259  0.136952 -1.725076 -0.062219   \n..        ...       ...       ...       ...       ...       ...       ...   \n803  0.733913 -0.355555  1.114178 -1.248123  0.704931 -0.277625 -0.795788   \n804 -0.294410 -0.696847 -0.007318 -0.339182 -0.553537  0.507754 -1.480299   \n805 -0.651554  1.469267 -0.270072 -0.080296 -1.391628  0.190607 -0.810352   \n806 -0.598115  0.430782 -0.907515  0.410136  0.022597  0.201202 -1.079199   \n807  1.669860  0.414430  0.865855  1.490688  2.058609  0.197576 -0.037285   \n\n            7         8         9  ...       451       452       453  \\\n0    0.158866  1.284785 -0.538130  ... -0.731349 -0.456020 -0.257331   \n1    1.688034 -0.593661  0.905911  ...  0.610428 -0.984907 -0.429806   \n2   -0.744261  0.031204  0.296597  ...  0.212365 -0.046016  1.147463   \n3    0.251138  1.613204  0.190688  ...  1.254787 -1.155922 -1.108540   \n4   -0.183102 -0.820445 -0.903953  ... -2.007721 -0.482311 -0.269142   \n..        ...       ...       ...  ...       ...       ...       ...   \n803  1.454845  0.919276  1.806882  ...  0.385552  0.065432  1.282541   \n804  1.433655 -0.809274  0.510881  ...  1.374670 -0.439340  0.070412   \n805 -1.689602 -0.625247 -0.250728  ...  1.906036  1.792039  0.773938   \n806 -1.474650 -1.188713 -0.556317  ... -0.669036  0.206248 -0.196512   \n807 -2.530671 -0.815114 -0.887634  ...  0.747081 -1.100277  0.964974   \n\n          454       455       456       457       458       459       460  \n0    0.396074  0.318007 -0.538754 -0.625193 -0.753419  0.154403  1.069385  \n1    0.199055  0.202587  1.612578  0.302153 -0.165713  0.905807  0.083180  \n2    0.696961 -0.574426  1.255969  0.270394  1.272939 -0.643112  0.433585  \n3   -2.046100  1.311100 -0.322965  0.999248 -1.238640  0.882844 -1.333590  \n4   -0.899796  1.083332  0.674665 -1.095657 -0.402669  0.677189  0.319992  \n..        ...       ...       ...       ...       ...       ...       ...  \n803  0.764427  0.781567  1.325098 -0.824188  0.841695  0.919510 -1.272032  \n804  0.303204 -0.368319  0.980816 -0.544742  0.108699  0.669852  0.930058  \n805 -1.201539 -0.959661 -1.606076  0.890055 -0.963963 -1.078870 -0.657256  \n806 -0.555876 -0.966467  0.532432 -0.678850  0.192851 -0.590212 -0.817717  \n807 -0.413041  0.789640 -0.790178  0.644039  0.984913  0.529260 -0.904156  \n\n[808 rows x 461 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.909515</td>\n      <td>0.388808</td>\n      <td>-1.535913</td>\n      <td>-0.133312</td>\n      <td>-1.826404</td>\n      <td>-0.532795</td>\n      <td>0.351273</td>\n      <td>0.158866</td>\n      <td>1.284785</td>\n      <td>-0.538130</td>\n      <td>...</td>\n      <td>-0.731349</td>\n      <td>-0.456020</td>\n      <td>-0.257331</td>\n      <td>0.396074</td>\n      <td>0.318007</td>\n      <td>-0.538754</td>\n      <td>-0.625193</td>\n      <td>-0.753419</td>\n      <td>0.154403</td>\n      <td>1.069385</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.107694</td>\n      <td>-0.097967</td>\n      <td>-0.539599</td>\n      <td>-0.331276</td>\n      <td>-0.942609</td>\n      <td>-0.054123</td>\n      <td>-1.212772</td>\n      <td>1.688034</td>\n      <td>-0.593661</td>\n      <td>0.905911</td>\n      <td>...</td>\n      <td>0.610428</td>\n      <td>-0.984907</td>\n      <td>-0.429806</td>\n      <td>0.199055</td>\n      <td>0.202587</td>\n      <td>1.612578</td>\n      <td>0.302153</td>\n      <td>-0.165713</td>\n      <td>0.905807</td>\n      <td>0.083180</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.092316</td>\n      <td>0.052596</td>\n      <td>-0.652025</td>\n      <td>1.218241</td>\n      <td>0.382968</td>\n      <td>-0.861838</td>\n      <td>-0.318937</td>\n      <td>-0.744261</td>\n      <td>0.031204</td>\n      <td>0.296597</td>\n      <td>...</td>\n      <td>0.212365</td>\n      <td>-0.046016</td>\n      <td>1.147463</td>\n      <td>0.696961</td>\n      <td>-0.574426</td>\n      <td>1.255969</td>\n      <td>0.270394</td>\n      <td>1.272939</td>\n      <td>-0.643112</td>\n      <td>0.433585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.119639</td>\n      <td>1.018918</td>\n      <td>3.687519</td>\n      <td>1.597563</td>\n      <td>0.055918</td>\n      <td>-1.406041</td>\n      <td>0.652994</td>\n      <td>0.251138</td>\n      <td>1.613204</td>\n      <td>0.190688</td>\n      <td>...</td>\n      <td>1.254787</td>\n      <td>-1.155922</td>\n      <td>-1.108540</td>\n      <td>-2.046100</td>\n      <td>1.311100</td>\n      <td>-0.322965</td>\n      <td>0.999248</td>\n      <td>-1.238640</td>\n      <td>0.882844</td>\n      <td>-1.333590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.109644</td>\n      <td>-0.290280</td>\n      <td>-0.278987</td>\n      <td>-0.603259</td>\n      <td>0.136952</td>\n      <td>-1.725076</td>\n      <td>-0.062219</td>\n      <td>-0.183102</td>\n      <td>-0.820445</td>\n      <td>-0.903953</td>\n      <td>...</td>\n      <td>-2.007721</td>\n      <td>-0.482311</td>\n      <td>-0.269142</td>\n      <td>-0.899796</td>\n      <td>1.083332</td>\n      <td>0.674665</td>\n      <td>-1.095657</td>\n      <td>-0.402669</td>\n      <td>0.677189</td>\n      <td>0.319992</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>803</th>\n      <td>0.733913</td>\n      <td>-0.355555</td>\n      <td>1.114178</td>\n      <td>-1.248123</td>\n      <td>0.704931</td>\n      <td>-0.277625</td>\n      <td>-0.795788</td>\n      <td>1.454845</td>\n      <td>0.919276</td>\n      <td>1.806882</td>\n      <td>...</td>\n      <td>0.385552</td>\n      <td>0.065432</td>\n      <td>1.282541</td>\n      <td>0.764427</td>\n      <td>0.781567</td>\n      <td>1.325098</td>\n      <td>-0.824188</td>\n      <td>0.841695</td>\n      <td>0.919510</td>\n      <td>-1.272032</td>\n    </tr>\n    <tr>\n      <th>804</th>\n      <td>-0.294410</td>\n      <td>-0.696847</td>\n      <td>-0.007318</td>\n      <td>-0.339182</td>\n      <td>-0.553537</td>\n      <td>0.507754</td>\n      <td>-1.480299</td>\n      <td>1.433655</td>\n      <td>-0.809274</td>\n      <td>0.510881</td>\n      <td>...</td>\n      <td>1.374670</td>\n      <td>-0.439340</td>\n      <td>0.070412</td>\n      <td>0.303204</td>\n      <td>-0.368319</td>\n      <td>0.980816</td>\n      <td>-0.544742</td>\n      <td>0.108699</td>\n      <td>0.669852</td>\n      <td>0.930058</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>-0.651554</td>\n      <td>1.469267</td>\n      <td>-0.270072</td>\n      <td>-0.080296</td>\n      <td>-1.391628</td>\n      <td>0.190607</td>\n      <td>-0.810352</td>\n      <td>-1.689602</td>\n      <td>-0.625247</td>\n      <td>-0.250728</td>\n      <td>...</td>\n      <td>1.906036</td>\n      <td>1.792039</td>\n      <td>0.773938</td>\n      <td>-1.201539</td>\n      <td>-0.959661</td>\n      <td>-1.606076</td>\n      <td>0.890055</td>\n      <td>-0.963963</td>\n      <td>-1.078870</td>\n      <td>-0.657256</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>-0.598115</td>\n      <td>0.430782</td>\n      <td>-0.907515</td>\n      <td>0.410136</td>\n      <td>0.022597</td>\n      <td>0.201202</td>\n      <td>-1.079199</td>\n      <td>-1.474650</td>\n      <td>-1.188713</td>\n      <td>-0.556317</td>\n      <td>...</td>\n      <td>-0.669036</td>\n      <td>0.206248</td>\n      <td>-0.196512</td>\n      <td>-0.555876</td>\n      <td>-0.966467</td>\n      <td>0.532432</td>\n      <td>-0.678850</td>\n      <td>0.192851</td>\n      <td>-0.590212</td>\n      <td>-0.817717</td>\n    </tr>\n    <tr>\n      <th>807</th>\n      <td>1.669860</td>\n      <td>0.414430</td>\n      <td>0.865855</td>\n      <td>1.490688</td>\n      <td>2.058609</td>\n      <td>0.197576</td>\n      <td>-0.037285</td>\n      <td>-2.530671</td>\n      <td>-0.815114</td>\n      <td>-0.887634</td>\n      <td>...</td>\n      <td>0.747081</td>\n      <td>-1.100277</td>\n      <td>0.964974</td>\n      <td>-0.413041</td>\n      <td>0.789640</td>\n      <td>-0.790178</td>\n      <td>0.644039</td>\n      <td>0.984913</td>\n      <td>0.529260</td>\n      <td>-0.904156</td>\n    </tr>\n  </tbody>\n</table>\n<p>808 rows × 461 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = X_train[X_train['date'] == 0][num_cols]\n",
    "test2.apply(zscore)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:21:40.429102800Z",
     "start_time": "2023-08-04T20:21:39.713813400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "      date                                                 id         0  \\\n808      1  a9580bb984c328091d2b70b497f97dce963bfd785621c8... -0.086588   \n809      1  56bccba09d92107ecf3af54246dcc504e059c6ea7917d6...  1.507991   \n810      1  0e3de002c861737e58efaed85626573c3d0ce8d6de9537... -1.238368   \n811      1  804e30556287959a5001b6714fb273269bbf714911ddf3... -0.284916   \n812      1  82c2cd4afa32c506fa06b81845fe7ce2ab6e701f7318cf...  0.350135   \n...    ...                                                ...       ...   \n1599     1  ccc024d9be136d7ff3052e3b180cc7b761046d7f938a09... -0.195073   \n1600     1  e4c21e9e63a03980770cf27027ee95cd5cb7c42523526e...  1.357609   \n1601     1  9e2c934ee7036e4c009f854cbad7c1918c15affa87de08...  0.992401   \n1602     1  4b66c74a9c71dff9bc7acf34704df667f6fdc2aa364c30...  0.327691   \n1603     1  76bf116e4e803ab83090712c79e0350f20d9cd21bf6de7...  0.113998   \n\n             1         2         3         4         5         6         7  \\\n808  -0.024100 -1.322844  1.055901 -1.636944  1.538946  0.856098  0.227508   \n809   0.007589  3.187609  0.773535  1.002994  2.231280  2.090351 -2.340482   \n810   0.902777 -1.155589  0.115469  0.298364 -0.657311 -0.331625  0.194307   \n811  -1.610276 -0.748725 -0.234539  0.705172 -0.413909 -0.166213  0.148394   \n812  -0.441179 -0.956842 -0.591735 -0.435247 -0.354551 -0.630023 -0.692771   \n...        ...       ...       ...       ...       ...       ...       ...   \n1599  0.020756 -1.394577 -0.400980 -0.074087  0.371736  0.155989  0.718443   \n1600 -0.182706 -0.755373 -0.433624  0.993479  1.136746 -1.422153  1.269599   \n1601  0.272522  0.686557 -0.594807  1.026931 -0.200617 -1.628415  1.007539   \n1602 -0.602777 -0.096060  0.146871 -0.208185  1.298718  0.184462 -4.727024   \n1603  0.817583 -0.384020 -0.345752 -0.124142  0.543419  0.183249  0.720322   \n\n      ...       451       452       453       454       455       456  \\\n808   ... -0.169484  0.374089 -1.130287 -0.914322  1.235445 -1.600928   \n809   ...  0.057639 -0.862123  2.334238 -0.914531  0.127877 -0.859922   \n810   ... -0.256396 -0.215674  0.113235  1.298021 -0.001681  0.862632   \n811   ... -1.137345  0.232178 -0.016697 -0.205444 -0.326632  0.712714   \n812   ... -1.489679  0.155139  0.277925 -0.515098  0.279647  0.894023   \n...   ...       ...       ...       ...       ...       ...       ...   \n1599  ... -0.784150 -1.078799 -0.237496  1.078066  1.261360  0.475074   \n1600  ... -1.224245 -0.731508  0.895141  1.018526 -0.991276  0.959560   \n1601  ... -0.599860  0.158135 -0.075477 -0.521898 -1.312801  0.690784   \n1602  ... -0.894132 -1.373807 -3.273008  0.991534  0.348748  0.326970   \n1603  ... -0.063700  0.430147  0.485960  0.891775  1.496141  0.011255   \n\n           457       458       459       460  \n808   2.636766 -1.796353 -0.251025  1.742490  \n809  -0.403767  1.211099  0.653786 -1.063859  \n810  -0.658961 -0.563213  1.409387 -0.159821  \n811   0.262151 -0.113863 -0.040705 -0.041048  \n812  -0.375981  1.336899 -0.588665  0.404817  \n...        ...       ...       ...       ...  \n1599 -1.200603 -1.135266  0.244244 -0.376367  \n1600 -1.100524 -1.275639  0.212083  0.041477  \n1601  0.316774  0.151364  0.984168  0.218821  \n1602  1.669505  1.092435  1.410759  0.882644  \n1603  0.023835 -1.146858  0.039979 -0.592262  \n\n[796 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>808</th>\n      <td>1</td>\n      <td>a9580bb984c328091d2b70b497f97dce963bfd785621c8...</td>\n      <td>-0.086588</td>\n      <td>-0.024100</td>\n      <td>-1.322844</td>\n      <td>1.055901</td>\n      <td>-1.636944</td>\n      <td>1.538946</td>\n      <td>0.856098</td>\n      <td>0.227508</td>\n      <td>...</td>\n      <td>-0.169484</td>\n      <td>0.374089</td>\n      <td>-1.130287</td>\n      <td>-0.914322</td>\n      <td>1.235445</td>\n      <td>-1.600928</td>\n      <td>2.636766</td>\n      <td>-1.796353</td>\n      <td>-0.251025</td>\n      <td>1.742490</td>\n    </tr>\n    <tr>\n      <th>809</th>\n      <td>1</td>\n      <td>56bccba09d92107ecf3af54246dcc504e059c6ea7917d6...</td>\n      <td>1.507991</td>\n      <td>0.007589</td>\n      <td>3.187609</td>\n      <td>0.773535</td>\n      <td>1.002994</td>\n      <td>2.231280</td>\n      <td>2.090351</td>\n      <td>-2.340482</td>\n      <td>...</td>\n      <td>0.057639</td>\n      <td>-0.862123</td>\n      <td>2.334238</td>\n      <td>-0.914531</td>\n      <td>0.127877</td>\n      <td>-0.859922</td>\n      <td>-0.403767</td>\n      <td>1.211099</td>\n      <td>0.653786</td>\n      <td>-1.063859</td>\n    </tr>\n    <tr>\n      <th>810</th>\n      <td>1</td>\n      <td>0e3de002c861737e58efaed85626573c3d0ce8d6de9537...</td>\n      <td>-1.238368</td>\n      <td>0.902777</td>\n      <td>-1.155589</td>\n      <td>0.115469</td>\n      <td>0.298364</td>\n      <td>-0.657311</td>\n      <td>-0.331625</td>\n      <td>0.194307</td>\n      <td>...</td>\n      <td>-0.256396</td>\n      <td>-0.215674</td>\n      <td>0.113235</td>\n      <td>1.298021</td>\n      <td>-0.001681</td>\n      <td>0.862632</td>\n      <td>-0.658961</td>\n      <td>-0.563213</td>\n      <td>1.409387</td>\n      <td>-0.159821</td>\n    </tr>\n    <tr>\n      <th>811</th>\n      <td>1</td>\n      <td>804e30556287959a5001b6714fb273269bbf714911ddf3...</td>\n      <td>-0.284916</td>\n      <td>-1.610276</td>\n      <td>-0.748725</td>\n      <td>-0.234539</td>\n      <td>0.705172</td>\n      <td>-0.413909</td>\n      <td>-0.166213</td>\n      <td>0.148394</td>\n      <td>...</td>\n      <td>-1.137345</td>\n      <td>0.232178</td>\n      <td>-0.016697</td>\n      <td>-0.205444</td>\n      <td>-0.326632</td>\n      <td>0.712714</td>\n      <td>0.262151</td>\n      <td>-0.113863</td>\n      <td>-0.040705</td>\n      <td>-0.041048</td>\n    </tr>\n    <tr>\n      <th>812</th>\n      <td>1</td>\n      <td>82c2cd4afa32c506fa06b81845fe7ce2ab6e701f7318cf...</td>\n      <td>0.350135</td>\n      <td>-0.441179</td>\n      <td>-0.956842</td>\n      <td>-0.591735</td>\n      <td>-0.435247</td>\n      <td>-0.354551</td>\n      <td>-0.630023</td>\n      <td>-0.692771</td>\n      <td>...</td>\n      <td>-1.489679</td>\n      <td>0.155139</td>\n      <td>0.277925</td>\n      <td>-0.515098</td>\n      <td>0.279647</td>\n      <td>0.894023</td>\n      <td>-0.375981</td>\n      <td>1.336899</td>\n      <td>-0.588665</td>\n      <td>0.404817</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>1</td>\n      <td>ccc024d9be136d7ff3052e3b180cc7b761046d7f938a09...</td>\n      <td>-0.195073</td>\n      <td>0.020756</td>\n      <td>-1.394577</td>\n      <td>-0.400980</td>\n      <td>-0.074087</td>\n      <td>0.371736</td>\n      <td>0.155989</td>\n      <td>0.718443</td>\n      <td>...</td>\n      <td>-0.784150</td>\n      <td>-1.078799</td>\n      <td>-0.237496</td>\n      <td>1.078066</td>\n      <td>1.261360</td>\n      <td>0.475074</td>\n      <td>-1.200603</td>\n      <td>-1.135266</td>\n      <td>0.244244</td>\n      <td>-0.376367</td>\n    </tr>\n    <tr>\n      <th>1600</th>\n      <td>1</td>\n      <td>e4c21e9e63a03980770cf27027ee95cd5cb7c42523526e...</td>\n      <td>1.357609</td>\n      <td>-0.182706</td>\n      <td>-0.755373</td>\n      <td>-0.433624</td>\n      <td>0.993479</td>\n      <td>1.136746</td>\n      <td>-1.422153</td>\n      <td>1.269599</td>\n      <td>...</td>\n      <td>-1.224245</td>\n      <td>-0.731508</td>\n      <td>0.895141</td>\n      <td>1.018526</td>\n      <td>-0.991276</td>\n      <td>0.959560</td>\n      <td>-1.100524</td>\n      <td>-1.275639</td>\n      <td>0.212083</td>\n      <td>0.041477</td>\n    </tr>\n    <tr>\n      <th>1601</th>\n      <td>1</td>\n      <td>9e2c934ee7036e4c009f854cbad7c1918c15affa87de08...</td>\n      <td>0.992401</td>\n      <td>0.272522</td>\n      <td>0.686557</td>\n      <td>-0.594807</td>\n      <td>1.026931</td>\n      <td>-0.200617</td>\n      <td>-1.628415</td>\n      <td>1.007539</td>\n      <td>...</td>\n      <td>-0.599860</td>\n      <td>0.158135</td>\n      <td>-0.075477</td>\n      <td>-0.521898</td>\n      <td>-1.312801</td>\n      <td>0.690784</td>\n      <td>0.316774</td>\n      <td>0.151364</td>\n      <td>0.984168</td>\n      <td>0.218821</td>\n    </tr>\n    <tr>\n      <th>1602</th>\n      <td>1</td>\n      <td>4b66c74a9c71dff9bc7acf34704df667f6fdc2aa364c30...</td>\n      <td>0.327691</td>\n      <td>-0.602777</td>\n      <td>-0.096060</td>\n      <td>0.146871</td>\n      <td>-0.208185</td>\n      <td>1.298718</td>\n      <td>0.184462</td>\n      <td>-4.727024</td>\n      <td>...</td>\n      <td>-0.894132</td>\n      <td>-1.373807</td>\n      <td>-3.273008</td>\n      <td>0.991534</td>\n      <td>0.348748</td>\n      <td>0.326970</td>\n      <td>1.669505</td>\n      <td>1.092435</td>\n      <td>1.410759</td>\n      <td>0.882644</td>\n    </tr>\n    <tr>\n      <th>1603</th>\n      <td>1</td>\n      <td>76bf116e4e803ab83090712c79e0350f20d9cd21bf6de7...</td>\n      <td>0.113998</td>\n      <td>0.817583</td>\n      <td>-0.384020</td>\n      <td>-0.345752</td>\n      <td>-0.124142</td>\n      <td>0.543419</td>\n      <td>0.183249</td>\n      <td>0.720322</td>\n      <td>...</td>\n      <td>-0.063700</td>\n      <td>0.430147</td>\n      <td>0.485960</td>\n      <td>0.891775</td>\n      <td>1.496141</td>\n      <td>0.011255</td>\n      <td>0.023835</td>\n      <td>-1.146858</td>\n      <td>0.039979</td>\n      <td>-0.592262</td>\n    </tr>\n  </tbody>\n</table>\n<p>796 rows × 463 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['date'] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:20:22.532252200Z",
     "start_time": "2023-08-04T20:20:22.499453200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "        date                                                 id         0  \\\n0          0  dae29c8061b3176b9208f26afbb96e2ca50886db41902d... -0.909515   \n1          0  2f71f1b5d49fbd131351df95848dc91ab14662af62d4d0... -0.107694   \n2          0  b8d41ef950b69f94c380410f59f47e15666c57b74573b6...  0.092316   \n3          0  cdce060d04ce28a551eaab653cc4b01f5ad878aeb932ec...  4.119639   \n4          0  86f6e6d9407ad3abfab91a3bbfb7ad71553e3f968765b8...  0.109644   \n...      ...                                                ...       ...   \n742665   268  5a18ddc0f252fa17cbd2a5bfe2f3786c0afb5052dd92be...  0.790984   \n742666   268  73c197cf1cb75641710562fe26d4f562c8228847a67949... -1.129492   \n742667   268  bad7ff9ebc5579589e5ef36cb58f962c90c864fd3dfb22...  1.656413   \n742668   268  5b968ca44ac0550be6f31470a96e572cd1c58d36cc26c7...  0.282704   \n742669   268  a42ec1ac915edb35b440184ca52015bf3fdba53c631b1f... -0.813073   \n\n               1         2         3         4         5         6         7  \\\n0       0.388808 -1.535913 -0.133312 -1.826404 -0.532795  0.351273  0.158866   \n1      -0.097967 -0.539599 -0.331276 -0.942609 -0.054123 -1.212772  1.688034   \n2       0.052596 -0.652025  1.218241  0.382968 -0.861838 -0.318937 -0.744261   \n3       1.018918  3.687519  1.597563  0.055918 -1.406041  0.652994  0.251138   \n4      -0.290280 -0.278987 -0.603259  0.136952 -1.725076 -0.062219 -0.183102   \n...          ...       ...       ...       ...       ...       ...       ...   \n742665  1.560877 -0.328996 -0.190068  0.314971 -0.001609  0.313957 -0.315743   \n742666  0.696247 -1.494771 -0.404022  0.909996 -0.658659  0.688591  1.634416   \n742667 -1.267060  0.748902 -0.196263  0.831206 -1.590837  3.079856  0.498583   \n742668  0.156104 -1.165022  0.513334 -1.111948 -1.368465 -1.347184 -0.926533   \n742669 -0.824916 -0.368725  0.136837  0.270865  0.710876  0.734015 -1.233695   \n\n        ...       451       452       453       454       455       456  \\\n0       ... -0.731349 -0.456020 -0.257331  0.396074  0.318007 -0.538754   \n1       ...  0.610428 -0.984907 -0.429806  0.199055  0.202587  1.612578   \n2       ...  0.212365 -0.046016  1.147463  0.696961 -0.574426  1.255969   \n3       ...  1.254787 -1.155922 -1.108540 -2.046100  1.311100 -0.322965   \n4       ... -2.007721 -0.482311 -0.269142 -0.899796  1.083332  0.674665   \n...     ...       ...       ...       ...       ...       ...       ...   \n742665  ... -1.450422 -1.044100  0.631455 -1.322626 -0.407846  0.578026   \n742666  ... -0.475011  0.319023 -1.038112  0.222924  0.804017 -0.969177   \n742667  ... -0.010330 -0.426130 -0.624393 -0.236483 -0.244052  1.280749   \n742668  ...  0.411093  0.225324 -0.112838 -0.366831 -0.385833 -0.301606   \n742669  ...  0.134728  0.133413 -0.904207 -0.430508 -1.598422 -0.819337   \n\n             457       458       459       460  \n0      -0.625193 -0.753419  0.154403  1.069385  \n1       0.302153 -0.165713  0.905807  0.083180  \n2       0.270394  1.272939 -0.643112  0.433585  \n3       0.999248 -1.238640  0.882844 -1.333590  \n4      -1.095657 -0.402669  0.677189  0.319992  \n...          ...       ...       ...       ...  \n742665  0.830650  1.414314 -0.845734  0.399335  \n742666 -1.011879 -0.921781 -0.067543  0.491890  \n742667 -2.001158 -1.036838 -1.959235 -2.534523  \n742668  0.395659 -0.895311 -0.819201 -0.996246  \n742669  0.012623  0.624302 -0.532539  0.105044  \n\n[742670 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>dae29c8061b3176b9208f26afbb96e2ca50886db41902d...</td>\n      <td>-0.909515</td>\n      <td>0.388808</td>\n      <td>-1.535913</td>\n      <td>-0.133312</td>\n      <td>-1.826404</td>\n      <td>-0.532795</td>\n      <td>0.351273</td>\n      <td>0.158866</td>\n      <td>...</td>\n      <td>-0.731349</td>\n      <td>-0.456020</td>\n      <td>-0.257331</td>\n      <td>0.396074</td>\n      <td>0.318007</td>\n      <td>-0.538754</td>\n      <td>-0.625193</td>\n      <td>-0.753419</td>\n      <td>0.154403</td>\n      <td>1.069385</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2f71f1b5d49fbd131351df95848dc91ab14662af62d4d0...</td>\n      <td>-0.107694</td>\n      <td>-0.097967</td>\n      <td>-0.539599</td>\n      <td>-0.331276</td>\n      <td>-0.942609</td>\n      <td>-0.054123</td>\n      <td>-1.212772</td>\n      <td>1.688034</td>\n      <td>...</td>\n      <td>0.610428</td>\n      <td>-0.984907</td>\n      <td>-0.429806</td>\n      <td>0.199055</td>\n      <td>0.202587</td>\n      <td>1.612578</td>\n      <td>0.302153</td>\n      <td>-0.165713</td>\n      <td>0.905807</td>\n      <td>0.083180</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>b8d41ef950b69f94c380410f59f47e15666c57b74573b6...</td>\n      <td>0.092316</td>\n      <td>0.052596</td>\n      <td>-0.652025</td>\n      <td>1.218241</td>\n      <td>0.382968</td>\n      <td>-0.861838</td>\n      <td>-0.318937</td>\n      <td>-0.744261</td>\n      <td>...</td>\n      <td>0.212365</td>\n      <td>-0.046016</td>\n      <td>1.147463</td>\n      <td>0.696961</td>\n      <td>-0.574426</td>\n      <td>1.255969</td>\n      <td>0.270394</td>\n      <td>1.272939</td>\n      <td>-0.643112</td>\n      <td>0.433585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>cdce060d04ce28a551eaab653cc4b01f5ad878aeb932ec...</td>\n      <td>4.119639</td>\n      <td>1.018918</td>\n      <td>3.687519</td>\n      <td>1.597563</td>\n      <td>0.055918</td>\n      <td>-1.406041</td>\n      <td>0.652994</td>\n      <td>0.251138</td>\n      <td>...</td>\n      <td>1.254787</td>\n      <td>-1.155922</td>\n      <td>-1.108540</td>\n      <td>-2.046100</td>\n      <td>1.311100</td>\n      <td>-0.322965</td>\n      <td>0.999248</td>\n      <td>-1.238640</td>\n      <td>0.882844</td>\n      <td>-1.333590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>86f6e6d9407ad3abfab91a3bbfb7ad71553e3f968765b8...</td>\n      <td>0.109644</td>\n      <td>-0.290280</td>\n      <td>-0.278987</td>\n      <td>-0.603259</td>\n      <td>0.136952</td>\n      <td>-1.725076</td>\n      <td>-0.062219</td>\n      <td>-0.183102</td>\n      <td>...</td>\n      <td>-2.007721</td>\n      <td>-0.482311</td>\n      <td>-0.269142</td>\n      <td>-0.899796</td>\n      <td>1.083332</td>\n      <td>0.674665</td>\n      <td>-1.095657</td>\n      <td>-0.402669</td>\n      <td>0.677189</td>\n      <td>0.319992</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>742665</th>\n      <td>268</td>\n      <td>5a18ddc0f252fa17cbd2a5bfe2f3786c0afb5052dd92be...</td>\n      <td>0.790984</td>\n      <td>1.560877</td>\n      <td>-0.328996</td>\n      <td>-0.190068</td>\n      <td>0.314971</td>\n      <td>-0.001609</td>\n      <td>0.313957</td>\n      <td>-0.315743</td>\n      <td>...</td>\n      <td>-1.450422</td>\n      <td>-1.044100</td>\n      <td>0.631455</td>\n      <td>-1.322626</td>\n      <td>-0.407846</td>\n      <td>0.578026</td>\n      <td>0.830650</td>\n      <td>1.414314</td>\n      <td>-0.845734</td>\n      <td>0.399335</td>\n    </tr>\n    <tr>\n      <th>742666</th>\n      <td>268</td>\n      <td>73c197cf1cb75641710562fe26d4f562c8228847a67949...</td>\n      <td>-1.129492</td>\n      <td>0.696247</td>\n      <td>-1.494771</td>\n      <td>-0.404022</td>\n      <td>0.909996</td>\n      <td>-0.658659</td>\n      <td>0.688591</td>\n      <td>1.634416</td>\n      <td>...</td>\n      <td>-0.475011</td>\n      <td>0.319023</td>\n      <td>-1.038112</td>\n      <td>0.222924</td>\n      <td>0.804017</td>\n      <td>-0.969177</td>\n      <td>-1.011879</td>\n      <td>-0.921781</td>\n      <td>-0.067543</td>\n      <td>0.491890</td>\n    </tr>\n    <tr>\n      <th>742667</th>\n      <td>268</td>\n      <td>bad7ff9ebc5579589e5ef36cb58f962c90c864fd3dfb22...</td>\n      <td>1.656413</td>\n      <td>-1.267060</td>\n      <td>0.748902</td>\n      <td>-0.196263</td>\n      <td>0.831206</td>\n      <td>-1.590837</td>\n      <td>3.079856</td>\n      <td>0.498583</td>\n      <td>...</td>\n      <td>-0.010330</td>\n      <td>-0.426130</td>\n      <td>-0.624393</td>\n      <td>-0.236483</td>\n      <td>-0.244052</td>\n      <td>1.280749</td>\n      <td>-2.001158</td>\n      <td>-1.036838</td>\n      <td>-1.959235</td>\n      <td>-2.534523</td>\n    </tr>\n    <tr>\n      <th>742668</th>\n      <td>268</td>\n      <td>5b968ca44ac0550be6f31470a96e572cd1c58d36cc26c7...</td>\n      <td>0.282704</td>\n      <td>0.156104</td>\n      <td>-1.165022</td>\n      <td>0.513334</td>\n      <td>-1.111948</td>\n      <td>-1.368465</td>\n      <td>-1.347184</td>\n      <td>-0.926533</td>\n      <td>...</td>\n      <td>0.411093</td>\n      <td>0.225324</td>\n      <td>-0.112838</td>\n      <td>-0.366831</td>\n      <td>-0.385833</td>\n      <td>-0.301606</td>\n      <td>0.395659</td>\n      <td>-0.895311</td>\n      <td>-0.819201</td>\n      <td>-0.996246</td>\n    </tr>\n    <tr>\n      <th>742669</th>\n      <td>268</td>\n      <td>a42ec1ac915edb35b440184ca52015bf3fdba53c631b1f...</td>\n      <td>-0.813073</td>\n      <td>-0.824916</td>\n      <td>-0.368725</td>\n      <td>0.136837</td>\n      <td>0.270865</td>\n      <td>0.710876</td>\n      <td>0.734015</td>\n      <td>-1.233695</td>\n      <td>...</td>\n      <td>0.134728</td>\n      <td>0.133413</td>\n      <td>-0.904207</td>\n      <td>-0.430508</td>\n      <td>-1.598422</td>\n      <td>-0.819337</td>\n      <td>0.012623</td>\n      <td>0.624302</td>\n      <td>-0.532539</td>\n      <td>0.105044</td>\n    </tr>\n  </tbody>\n</table>\n<p>742670 rows × 463 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:19:37.890501400Z",
     "start_time": "2023-08-04T20:19:37.787607400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: >"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = X_train[X_train['date'] == 0][num_cols]\n",
    "\n",
    "test.apply(zscore)['0'].hist(bins=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:27:08.793793900Z",
     "start_time": "2023-08-04T20:27:08.028583300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: >"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['0'].hist(bins=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T20:26:15.919123200Z",
     "start_time": "2023-08-04T20:26:15.777611500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
