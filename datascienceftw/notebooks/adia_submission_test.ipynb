{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-25T18:07:23.274302600Z",
     "start_time": "2023-07-25T18:07:20.631219400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-07-25T18:07:24.312950200Z",
     "start_time": "2023-07-25T18:07:24.302450300Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def convert_to_pairwise_train(X_train, y_train):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    n_samples = X_train.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_train[i, 2:], X_train[j, 2:]])\n",
    "            ids.append([X_train[i, :2], X_train[j, :2]])\n",
    "            labels.append(1 if y_train[i] > y_train[j] else 0)\n",
    "    return np.array(pairs).astype('float32'), np.array(labels).astype('float32'), np.array(ids)\n",
    "\n",
    "def convert_to_pairwise_test(X_test):\n",
    "    pairs = []\n",
    "    ids = []\n",
    "    n_samples = X_test.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            pairs.append([X_test[i, 2:], X_test[j, 2:]])\n",
    "            ids.append([X_test[i, :2], X_test[j, :2]])\n",
    "    return np.array(pairs).astype('float32'), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    X_train_orig = X_train.copy()\n",
    "    y_train_orig = y_train.copy()\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train[['date', 'id']])\n",
    "    X_scale_pca = X_train.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    #Begin Dates Processing\n",
    "    date_list = list(set(X_train_concat[:,0]))\n",
    "    dates_array = list(split(date_list, 20))\n",
    "\n",
    "\n",
    "    for dates_list in dates_array:\n",
    "        print(dates_list)\n",
    "        X_train_pairs = np.empty((0, 2, 40))\n",
    "        y_train_labels = np.empty((0,))\n",
    "        X_train_ids = np.empty((0, 2, 2))\n",
    "\n",
    "        for date in dates_list:\n",
    "            X_for_pairs = X_train_concat[X_train_concat[:,0] == date]\n",
    "            y_for_pairs = y_train[y_train[:,0] == date][:,2]\n",
    "            X_train_pair_array, y_train_labels_array, X_train_ids_array = convert_to_pairwise_train(X_for_pairs, y_for_pairs)\n",
    "\n",
    "            X_train_pairs = np.concatenate((X_train_pairs, X_train_pair_array), axis=0)\n",
    "            y_train_labels = np.concatenate((y_train_labels, y_train_labels_array), axis=0)\n",
    "            X_train_ids = np.concatenate((X_train_ids, X_train_ids_array), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        #Train Test Split\n",
    "        X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_train_pairs, y_train_labels, random_state=42, shuffle=True, test_size=0.3)\n",
    "        del X_train_pairs\n",
    "        del y_train_labels\n",
    "\n",
    "\n",
    "\n",
    "        #Model Training\n",
    "        model_pathname = Path('resources') / \"model.keras\"\n",
    "\n",
    "        if model_pathname.is_file():\n",
    "            print(f\"Opened Model for Date {date}\")\n",
    "\n",
    "            model = load_model(model_pathname)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_nn,\n",
    "                y_train_nn,\n",
    "                batch_size=5000,\n",
    "                epochs=10,\n",
    "                validation_data=[X_test_nn, y_test_nn],\n",
    "                callbacks=[mc, early_stopping],\n",
    "                shuffle=False,\n",
    "                use_multiprocessing=True,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            #Neural Network Model\n",
    "            mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=1,\n",
    "                verbose=0,\n",
    "                mode='auto',\n",
    "                baseline=None,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(800, activation='relu', kernel_initializer='lecun_normal', input_shape=(X_train_nn.shape[1], X_train_nn.shape[2])),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(500, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Flatten(),\n",
    "                keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "            ])\n",
    "\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "            model.compile(optimizer=optimizer,\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train_nn,\n",
    "                y_train_nn,\n",
    "                batch_size=10000,\n",
    "                epochs=10,\n",
    "                validation_data=[X_test_nn, y_test_nn],\n",
    "                callbacks=[mc, early_stopping],\n",
    "                shuffle=True,\n",
    "                use_multiprocessing=True,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            model.save(model_pathname)\n",
    "\n",
    "            del X_train_nn\n",
    "            del X_test_nn\n",
    "            del y_train_nn\n",
    "            del y_test_nn\n",
    "\n",
    "        print(f\"Finished training for Date {dates_list}\")\n",
    "    print(\"Finished All Training\")\n",
    "\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T18:09:31.651760400Z",
     "start_time": "2023-07-25T18:09:31.638749Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-07-25T18:09:32.209856300Z",
     "start_time": "2023-07-25T18:09:32.201354100Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "    dates = list(X_test_orig['date'].unique())\n",
    "\n",
    "    #Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    #Load PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "\n",
    "    #Scaling\n",
    "    X_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    pca_features = pca.transform(X_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    for date in dates:\n",
    "        X_test_date = X_test_orig[X_test_orig['date'] == date]\n",
    "        X_for_pairs = X_test_concat[X_test_concat[:,0] == date]\n",
    "\n",
    "        #Load Model\n",
    "        model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "        model = load_model(model_pathname)\n",
    "\n",
    "        #Pairwise Transformation\n",
    "        X_test_pairs, X_test_ids = convert_to_pairwise_test(X_for_pairs)\n",
    "\n",
    "        print(f\"Predicting for Date {date} in Test\")\n",
    "        preds = model.predict(X_test_pairs, batch_size=3000)\n",
    "\n",
    "        preds_df_1 = pd.DataFrame({'id': X_test_ids[:,0,1].flatten(), 'date': X_test_ids[:,0,0].flatten(), 'value': preds.flatten()})\n",
    "\n",
    "        result = preds_df_1.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "        result = pd.merge(X_test_date, result, on=['id', 'date'], how='left')\n",
    "\n",
    "        result = result[['date', 'id', 'value']]\n",
    "\n",
    "        result['value'] = result['value'].fillna(result['value'].mean())\n",
    "\n",
    "        minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "        # Scale the 'Values' column\n",
    "        result['value'] = minmax.fit_transform(result[['value']])\n",
    "\n",
    "        result_df = pd.concat([result_df, result], ignore_index=False, axis=0)\n",
    "\n",
    "        print(f\"Finished predictions for Date {date} in Test\")\n",
    "    print(\"Finished All Predictions\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T18:09:34.913055600Z",
     "start_time": "2023-07-25T18:09:33.370580100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T18:10:27.355521300Z",
     "start_time": "2023-07-25T18:09:37.625571700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
