{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:30.242497200Z",
     "start_time": "2023-08-04T21:53:26.549704600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "##### FUNCTIONS #####\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Create a dictionary to store indices for each date\n",
    "    date_indices = defaultdict(list)\n",
    "    for i in range(n_samples):\n",
    "        date_indices[X[i, 0]].append(i)\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        generated_pairs = set()  # Reset generated pairs for each batch\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = date_indices[date_i]\n",
    "            for j in same_date_indices:\n",
    "                if i >= j:  # Ignore duplicate pairs and self-pairs\n",
    "                    continue\n",
    "                pair_key = tuple(sorted([i, j]))  # Use sorted indices as a unique key\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = np.array([batch_X[i, 2:], batch_X[j, 2:]])\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                if len(X_batch) == batch_size:\n",
    "                    X_batch_array = np.array(X_batch, dtype='float32')\n",
    "                    yield [X_batch_array[:, 0], X_batch_array[:, 1]], np.array(y_batch, dtype='float32').reshape(-1,1)\n",
    "                    X_batch, y_batch = [], []  # Reset batches\n",
    "\n",
    "\n",
    "def create_siamese_model(input_shape):\n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "    # Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Generate the encodings for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    # Return the model\n",
    "    return siamese_net"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:31.058482100Z",
     "start_time": "2023-08-04T21:53:31.052753500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "##### TRAIN #####\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 0\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch = 1000,\n",
    "            epochs=1000,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = 500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = create_siamese_model((X_train_concat.shape[1] - 2,))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['auc'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1000,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:31.640358600Z",
     "start_time": "2023-08-04T21:53:31.626892100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    X_test_concat = np.concatenate((X_ids, X_scale_pca), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 1000\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:32.211838900Z",
     "start_time": "2023-08-04T21:53:32.205605700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:34.462069500Z",
     "start_time": "2023-08-04T21:53:32.777921800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737864, 463)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 53\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(X_train, y_train, model_directory_path)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_pathname\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m     46\u001B[0m     early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(\n\u001B[0;32m     47\u001B[0m         monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     48\u001B[0m         patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m     49\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     50\u001B[0m         mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     51\u001B[0m         baseline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 53\u001B[0m     history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m     54\u001B[0m         train_generator,\n\u001B[0;32m     55\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     56\u001B[0m         steps_per_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m     57\u001B[0m         epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m     58\u001B[0m         validation_data\u001B[38;5;241m=\u001B[39mtest_generator,\n\u001B[0;32m     59\u001B[0m         validation_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m,\n\u001B[0;32m     60\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39m[mc, early_stopping],\n\u001B[0;32m     61\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     62\u001B[0m         use_multiprocessing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     63\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     64\u001B[0m     )\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m#Neural Network Model\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     mc \u001B[38;5;241m=\u001B[39m ModelCheckpoint(model_pathname, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:53:48.474320100Z",
     "start_time": "2023-08-04T21:53:34.463072500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "        date                                                 id         0  \\\n0          0  dae29c8061b3176b9208f26afbb96e2ca50886db41902d... -0.909515   \n1          0  2f71f1b5d49fbd131351df95848dc91ab14662af62d4d0... -0.107694   \n2          0  b8d41ef950b69f94c380410f59f47e15666c57b74573b6...  0.092316   \n3          0  cdce060d04ce28a551eaab653cc4b01f5ad878aeb932ec...  4.119639   \n4          0  86f6e6d9407ad3abfab91a3bbfb7ad71553e3f968765b8...  0.109644   \n...      ...                                                ...       ...   \n742665   268  5a18ddc0f252fa17cbd2a5bfe2f3786c0afb5052dd92be...  0.790984   \n742666   268  73c197cf1cb75641710562fe26d4f562c8228847a67949... -1.129492   \n742667   268  bad7ff9ebc5579589e5ef36cb58f962c90c864fd3dfb22...  1.656413   \n742668   268  5b968ca44ac0550be6f31470a96e572cd1c58d36cc26c7...  0.282704   \n742669   268  a42ec1ac915edb35b440184ca52015bf3fdba53c631b1f... -0.813073   \n\n               1         2         3         4         5         6         7  \\\n0       0.388808 -1.535913 -0.133312 -1.826404 -0.532795  0.351273  0.158866   \n1      -0.097967 -0.539599 -0.331276 -0.942609 -0.054123 -1.212772  1.688034   \n2       0.052596 -0.652025  1.218241  0.382968 -0.861838 -0.318937 -0.744261   \n3       1.018918  3.687519  1.597563  0.055918 -1.406041  0.652994  0.251138   \n4      -0.290280 -0.278987 -0.603259  0.136952 -1.725076 -0.062219 -0.183102   \n...          ...       ...       ...       ...       ...       ...       ...   \n742665  1.560877 -0.328996 -0.190068  0.314971 -0.001609  0.313957 -0.315743   \n742666  0.696247 -1.494771 -0.404022  0.909996 -0.658659  0.688591  1.634416   \n742667 -1.267060  0.748902 -0.196263  0.831206 -1.590837  3.079856  0.498583   \n742668  0.156104 -1.165022  0.513334 -1.111948 -1.368465 -1.347184 -0.926533   \n742669 -0.824916 -0.368725  0.136837  0.270865  0.710876  0.734015 -1.233695   \n\n        ...       451       452       453       454       455       456  \\\n0       ... -0.731349 -0.456020 -0.257331  0.396074  0.318007 -0.538754   \n1       ...  0.610428 -0.984907 -0.429806  0.199055  0.202587  1.612578   \n2       ...  0.212365 -0.046016  1.147463  0.696961 -0.574426  1.255969   \n3       ...  1.254787 -1.155922 -1.108540 -2.046100  1.311100 -0.322965   \n4       ... -2.007721 -0.482311 -0.269142 -0.899796  1.083332  0.674665   \n...     ...       ...       ...       ...       ...       ...       ...   \n742665  ... -1.450422 -1.044100  0.631455 -1.322626 -0.407846  0.578026   \n742666  ... -0.475011  0.319023 -1.038112  0.222924  0.804017 -0.969177   \n742667  ... -0.010330 -0.426130 -0.624393 -0.236483 -0.244052  1.280749   \n742668  ...  0.411093  0.225324 -0.112838 -0.366831 -0.385833 -0.301606   \n742669  ...  0.134728  0.133413 -0.904207 -0.430508 -1.598422 -0.819337   \n\n             457       458       459       460  \n0      -0.625193 -0.753419  0.154403  1.069385  \n1       0.302153 -0.165713  0.905807  0.083180  \n2       0.270394  1.272939 -0.643112  0.433585  \n3       0.999248 -1.238640  0.882844 -1.333590  \n4      -1.095657 -0.402669  0.677189  0.319992  \n...          ...       ...       ...       ...  \n742665  0.830650  1.414314 -0.845734  0.399335  \n742666 -1.011879 -0.921781 -0.067543  0.491890  \n742667 -2.001158 -1.036838 -1.959235 -2.534523  \n742668  0.395659 -0.895311 -0.819201 -0.996246  \n742669  0.012623  0.624302 -0.532539  0.105044  \n\n[742670 rows x 463 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>451</th>\n      <th>452</th>\n      <th>453</th>\n      <th>454</th>\n      <th>455</th>\n      <th>456</th>\n      <th>457</th>\n      <th>458</th>\n      <th>459</th>\n      <th>460</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>dae29c8061b3176b9208f26afbb96e2ca50886db41902d...</td>\n      <td>-0.909515</td>\n      <td>0.388808</td>\n      <td>-1.535913</td>\n      <td>-0.133312</td>\n      <td>-1.826404</td>\n      <td>-0.532795</td>\n      <td>0.351273</td>\n      <td>0.158866</td>\n      <td>...</td>\n      <td>-0.731349</td>\n      <td>-0.456020</td>\n      <td>-0.257331</td>\n      <td>0.396074</td>\n      <td>0.318007</td>\n      <td>-0.538754</td>\n      <td>-0.625193</td>\n      <td>-0.753419</td>\n      <td>0.154403</td>\n      <td>1.069385</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2f71f1b5d49fbd131351df95848dc91ab14662af62d4d0...</td>\n      <td>-0.107694</td>\n      <td>-0.097967</td>\n      <td>-0.539599</td>\n      <td>-0.331276</td>\n      <td>-0.942609</td>\n      <td>-0.054123</td>\n      <td>-1.212772</td>\n      <td>1.688034</td>\n      <td>...</td>\n      <td>0.610428</td>\n      <td>-0.984907</td>\n      <td>-0.429806</td>\n      <td>0.199055</td>\n      <td>0.202587</td>\n      <td>1.612578</td>\n      <td>0.302153</td>\n      <td>-0.165713</td>\n      <td>0.905807</td>\n      <td>0.083180</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>b8d41ef950b69f94c380410f59f47e15666c57b74573b6...</td>\n      <td>0.092316</td>\n      <td>0.052596</td>\n      <td>-0.652025</td>\n      <td>1.218241</td>\n      <td>0.382968</td>\n      <td>-0.861838</td>\n      <td>-0.318937</td>\n      <td>-0.744261</td>\n      <td>...</td>\n      <td>0.212365</td>\n      <td>-0.046016</td>\n      <td>1.147463</td>\n      <td>0.696961</td>\n      <td>-0.574426</td>\n      <td>1.255969</td>\n      <td>0.270394</td>\n      <td>1.272939</td>\n      <td>-0.643112</td>\n      <td>0.433585</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>cdce060d04ce28a551eaab653cc4b01f5ad878aeb932ec...</td>\n      <td>4.119639</td>\n      <td>1.018918</td>\n      <td>3.687519</td>\n      <td>1.597563</td>\n      <td>0.055918</td>\n      <td>-1.406041</td>\n      <td>0.652994</td>\n      <td>0.251138</td>\n      <td>...</td>\n      <td>1.254787</td>\n      <td>-1.155922</td>\n      <td>-1.108540</td>\n      <td>-2.046100</td>\n      <td>1.311100</td>\n      <td>-0.322965</td>\n      <td>0.999248</td>\n      <td>-1.238640</td>\n      <td>0.882844</td>\n      <td>-1.333590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>86f6e6d9407ad3abfab91a3bbfb7ad71553e3f968765b8...</td>\n      <td>0.109644</td>\n      <td>-0.290280</td>\n      <td>-0.278987</td>\n      <td>-0.603259</td>\n      <td>0.136952</td>\n      <td>-1.725076</td>\n      <td>-0.062219</td>\n      <td>-0.183102</td>\n      <td>...</td>\n      <td>-2.007721</td>\n      <td>-0.482311</td>\n      <td>-0.269142</td>\n      <td>-0.899796</td>\n      <td>1.083332</td>\n      <td>0.674665</td>\n      <td>-1.095657</td>\n      <td>-0.402669</td>\n      <td>0.677189</td>\n      <td>0.319992</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>742665</th>\n      <td>268</td>\n      <td>5a18ddc0f252fa17cbd2a5bfe2f3786c0afb5052dd92be...</td>\n      <td>0.790984</td>\n      <td>1.560877</td>\n      <td>-0.328996</td>\n      <td>-0.190068</td>\n      <td>0.314971</td>\n      <td>-0.001609</td>\n      <td>0.313957</td>\n      <td>-0.315743</td>\n      <td>...</td>\n      <td>-1.450422</td>\n      <td>-1.044100</td>\n      <td>0.631455</td>\n      <td>-1.322626</td>\n      <td>-0.407846</td>\n      <td>0.578026</td>\n      <td>0.830650</td>\n      <td>1.414314</td>\n      <td>-0.845734</td>\n      <td>0.399335</td>\n    </tr>\n    <tr>\n      <th>742666</th>\n      <td>268</td>\n      <td>73c197cf1cb75641710562fe26d4f562c8228847a67949...</td>\n      <td>-1.129492</td>\n      <td>0.696247</td>\n      <td>-1.494771</td>\n      <td>-0.404022</td>\n      <td>0.909996</td>\n      <td>-0.658659</td>\n      <td>0.688591</td>\n      <td>1.634416</td>\n      <td>...</td>\n      <td>-0.475011</td>\n      <td>0.319023</td>\n      <td>-1.038112</td>\n      <td>0.222924</td>\n      <td>0.804017</td>\n      <td>-0.969177</td>\n      <td>-1.011879</td>\n      <td>-0.921781</td>\n      <td>-0.067543</td>\n      <td>0.491890</td>\n    </tr>\n    <tr>\n      <th>742667</th>\n      <td>268</td>\n      <td>bad7ff9ebc5579589e5ef36cb58f962c90c864fd3dfb22...</td>\n      <td>1.656413</td>\n      <td>-1.267060</td>\n      <td>0.748902</td>\n      <td>-0.196263</td>\n      <td>0.831206</td>\n      <td>-1.590837</td>\n      <td>3.079856</td>\n      <td>0.498583</td>\n      <td>...</td>\n      <td>-0.010330</td>\n      <td>-0.426130</td>\n      <td>-0.624393</td>\n      <td>-0.236483</td>\n      <td>-0.244052</td>\n      <td>1.280749</td>\n      <td>-2.001158</td>\n      <td>-1.036838</td>\n      <td>-1.959235</td>\n      <td>-2.534523</td>\n    </tr>\n    <tr>\n      <th>742668</th>\n      <td>268</td>\n      <td>5b968ca44ac0550be6f31470a96e572cd1c58d36cc26c7...</td>\n      <td>0.282704</td>\n      <td>0.156104</td>\n      <td>-1.165022</td>\n      <td>0.513334</td>\n      <td>-1.111948</td>\n      <td>-1.368465</td>\n      <td>-1.347184</td>\n      <td>-0.926533</td>\n      <td>...</td>\n      <td>0.411093</td>\n      <td>0.225324</td>\n      <td>-0.112838</td>\n      <td>-0.366831</td>\n      <td>-0.385833</td>\n      <td>-0.301606</td>\n      <td>0.395659</td>\n      <td>-0.895311</td>\n      <td>-0.819201</td>\n      <td>-0.996246</td>\n    </tr>\n    <tr>\n      <th>742669</th>\n      <td>268</td>\n      <td>a42ec1ac915edb35b440184ca52015bf3fdba53c631b1f...</td>\n      <td>-0.813073</td>\n      <td>-0.824916</td>\n      <td>-0.368725</td>\n      <td>0.136837</td>\n      <td>0.270865</td>\n      <td>0.710876</td>\n      <td>0.734015</td>\n      <td>-1.233695</td>\n      <td>...</td>\n      <td>0.134728</td>\n      <td>0.133413</td>\n      <td>-0.904207</td>\n      <td>-0.430508</td>\n      <td>-1.598422</td>\n      <td>-0.819337</td>\n      <td>0.012623</td>\n      <td>0.624302</td>\n      <td>-0.532539</td>\n      <td>0.105044</td>\n    </tr>\n  </tbody>\n</table>\n<p>742670 rows Ã— 463 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T21:50:43.405884100Z",
     "start_time": "2023-08-04T21:50:43.237958800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
