{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:02:50.926839800Z",
     "start_time": "2023-08-04T23:02:47.492985500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "##### FUNCTIONS #####\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Create a dictionary to store indices for each date\n",
    "    date_indices = defaultdict(list)\n",
    "    for i in range(n_samples):\n",
    "        date_indices[X[i, 0]].append(i)\n",
    "\n",
    "    while True:\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for date_i in date_indices.keys():\n",
    "            same_date_indices = date_indices[date_i]\n",
    "\n",
    "            if len(same_date_indices) < 2:\n",
    "                continue\n",
    "\n",
    "            # Create pairs for all samples of the same date\n",
    "            for i in range(len(same_date_indices)):\n",
    "                for j in range(i+1, len(same_date_indices)):\n",
    "                    before_comparison = y[same_date_indices]\n",
    "                    # Ensure the first sample in each pair is the one with the higher target value\n",
    "                    if y[same_date_indices[i]][2] > y[same_date_indices[j]][2]:\n",
    "                        X_pair = [X[same_date_indices[i], 2:], X[same_date_indices[j], 2:]]  # Ignore the first two columns\n",
    "                        y_pair = 1\n",
    "                    else:\n",
    "                        X_pair = [X[same_date_indices[j], 2:], X[same_date_indices[i], 2:]]  # Ignore the first two columns\n",
    "                        y_pair = 0\n",
    "\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        X_batch_array = np.array(X_batch, dtype='float32')\n",
    "                        yield [X_batch_array[:, 0], X_batch_array[:, 1]], np.array(y_batch, dtype='float32').reshape(-1,1)\n",
    "                        X_batch = []\n",
    "                        y_batch = []  # Reset batches\n",
    "\n",
    "        if X_batch:  # If there are any remaining samples that didn't make a full batch\n",
    "            X_batch_array = np.array(X_batch, dtype='float32')\n",
    "            yield [X_batch_array[:, 0], X_batch_array[:, 1]], np.array(y_batch, dtype='float32').reshape(-1,1)\n",
    "\n",
    "\n",
    "def create_siamese_model(input_shape):\n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "    # Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    # Generate the encodings for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    # Return the model\n",
    "    return siamese_net"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:17:09.682154900Z",
     "start_time": "2023-08-04T23:17:09.673715600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "##### TRAIN #####\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 0\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = np.asarray(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = np.asarray(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 120\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 3500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch = 1000,\n",
    "            epochs=1000,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = 500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = create_siamese_model((X_train_concat.shape[1] - 2))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=[AUC(name='auc')])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1000,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:17:11.388892400Z",
     "start_time": "2023-08-04T23:17:11.380328500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    X_test_concat = np.concatenate((X_ids, X_scale_pca), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 3500\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:17:12.578115600Z",
     "start_time": "2023-08-04T23:17:12.572341400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:17:15.134729100Z",
     "start_time": "2023-08-04T23:17:13.480159800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737864, 122)\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6961 - auc: 0.5061\n",
      "Epoch 1: val_loss improved from inf to 0.69313, saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 0.6961 - auc: 0.5061 - val_loss: 0.6931 - val_auc: 0.4985\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6939 - auc: 0.5002\n",
      "Epoch 2: val_loss did not improve from 0.69313\n",
      "1000/1000 [==============================] - 565s 566ms/step - loss: 0.6939 - auc: 0.5002 - val_loss: 0.6933 - val_auc: 0.4987\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6944 - auc: 0.4980\n",
      "Epoch 3: val_loss did not improve from 0.69313\n",
      "1000/1000 [==============================] - 576s 576ms/step - loss: 0.6944 - auc: 0.4980 - val_loss: 0.6943 - val_auc: 0.5040\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6936 - auc: 0.5019\n",
      "Epoch 4: val_loss did not improve from 0.69313\n",
      "1000/1000 [==============================] - 591s 592ms/step - loss: 0.6936 - auc: 0.5019 - val_loss: 0.6934 - val_auc: 0.5038\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6934 - auc: 0.4994\n",
      "Epoch 5: val_loss did not improve from 0.69313\n",
      "1000/1000 [==============================] - 619s 620ms/step - loss: 0.6934 - auc: 0.4994 - val_loss: 0.6934 - val_auc: 0.4973\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6934 - auc: 0.4897\n",
      "Epoch 6: val_loss improved from 0.69313 to 0.69308, saving model to ..\\resources\\model.keras\n",
      "1000/1000 [==============================] - 631s 632ms/step - loss: 0.6934 - auc: 0.4897 - val_loss: 0.6931 - val_auc: 0.4989\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6940 - auc: 0.4972\n",
      "Epoch 7: val_loss did not improve from 0.69308\n",
      "1000/1000 [==============================] - 634s 635ms/step - loss: 0.6940 - auc: 0.4972 - val_loss: 0.6933 - val_auc: 0.4996\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6931 - auc: 0.4946\n",
      "Epoch 8: val_loss did not improve from 0.69308\n",
      "1000/1000 [==============================] - 658s 659ms/step - loss: 0.6931 - auc: 0.4946 - val_loss: 0.6934 - val_auc: 0.5041\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6935 - auc: 0.4966\n",
      "Epoch 9: val_loss did not improve from 0.69308\n",
      "1000/1000 [==============================] - 649s 650ms/step - loss: 0.6935 - auc: 0.4966 - val_loss: 0.6932 - val_auc: 0.4968\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6932 - auc: 0.5000\n",
      "Epoch 10: val_loss did not improve from 0.69308\n",
      "1000/1000 [==============================] - 628s 629ms/step - loss: 0.6932 - auc: 0.5000 - val_loss: 0.6937 - val_auc: 0.5078\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6933 - auc: 0.4933\n",
      "Epoch 11: val_loss did not improve from 0.69308\n",
      "1000/1000 [==============================] - 649s 649ms/step - loss: 0.6933 - auc: 0.4933 - val_loss: 0.6931 - val_auc: 0.4929\n",
      "Epoch 12/1000\n",
      " 965/1000 [===========================>..] - ETA: 27:28 - loss: 0.6933 - auc: 0.4910"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[58], line 88\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(X_train, y_train, model_directory_path)\u001B[0m\n\u001B[0;32m     82\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m     84\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m     85\u001B[0m                   loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     86\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[AUC(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m'\u001B[39m)])\n\u001B[1;32m---> 88\u001B[0m     history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mmc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m     model\u001B[38;5;241m.\u001B[39msave(model_pathname)\n\u001B[0;32m    106\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1570\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1568\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[0;32m   1569\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1570\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1572\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:470\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 470\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:317\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 317\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:340\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    337\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    343\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:388\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    387\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 388\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:1081\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m-> 1081\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:1158\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[0;32m   1157\u001B[0m     logs \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39msync_to_numpy_or_python_type(logs)\n\u001B[1;32m-> 1158\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogbar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:1051\u001B[0m, in \u001B[0;36mProgbar.update\u001B[1;34m(self, current, values, finalize)\u001B[0m\n\u001B[0;32m   1048\u001B[0m         info \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1050\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m info\n\u001B[1;32m-> 1051\u001B[0m     \u001B[43mio_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_msg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_break\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py:80\u001B[0m, in \u001B[0;36mprint_msg\u001B[1;34m(message, line_break)\u001B[0m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     79\u001B[0m         sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mwrite(message)\n\u001B[1;32m---> 80\u001B[0m     \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     82\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(message)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:486\u001B[0m, in \u001B[0;36mOutStream.flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# wait for flush to actually get through, if we can.\u001B[39;00m\n\u001B[0;32m    485\u001B[0m evt \u001B[38;5;241m=\u001B[39m threading\u001B[38;5;241m.\u001B[39mEvent()\n\u001B[1;32m--> 486\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpub_thread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m evt\u001B[38;5;241m.\u001B[39mwait(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflush_timeout):\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:210\u001B[0m, in \u001B[0;36mIOPubThread.schedule\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_events\u001B[38;5;241m.\u001B[39mappend(f)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;66;03m# wake event thread (message content is ignored)\u001B[39;00m\n\u001B[1;32m--> 210\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event_pipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    212\u001B[0m     f()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py:618\u001B[0m, in \u001B[0;36mSocket.send\u001B[1;34m(self, data, flags, copy, track, routing_id, group)\u001B[0m\n\u001B[0;32m    611\u001B[0m         data \u001B[38;5;241m=\u001B[39m zmq\u001B[38;5;241m.\u001B[39mFrame(\n\u001B[0;32m    612\u001B[0m             data,\n\u001B[0;32m    613\u001B[0m             track\u001B[38;5;241m=\u001B[39mtrack,\n\u001B[0;32m    614\u001B[0m             copy\u001B[38;5;241m=\u001B[39mcopy \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    615\u001B[0m             copy_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy_threshold,\n\u001B[0;32m    616\u001B[0m         )\n\u001B[0;32m    617\u001B[0m     data\u001B[38;5;241m.\u001B[39mgroup \u001B[38;5;241m=\u001B[39m group\n\u001B[1;32m--> 618\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrack\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:740\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:787\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:244\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket._send_copy\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001B[0m, in \u001B[0;36mzmq.backend.cython.checkrc._check_rc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T13:46:39.751815200Z",
     "start_time": "2023-08-04T23:17:16.115801600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')\n",
    "\n",
    "model_directory_path = '../resources'\n",
    "max_date = X_train['date'].max()\n",
    "min_date = 0\n",
    "X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "X_test = X_train[X_train['date'] == max_date]\n",
    "y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "#PCA\n",
    "n_components = 120\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_features = pca.fit_transform(X_scale_pca)\n",
    "X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "pca_features_test = pca.transform(X_test_scale_pca)\n",
    "X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "#Save out Scaler and PCA\n",
    "with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "batch_size = 500\n",
    "train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "test_generator= pairwise_generator5(X_train_concat, y_train, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:03:11.439917100Z",
     "start_time": "2023-08-04T23:02:58.433179100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "x, y = next(train_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:05:25.544692400Z",
     "start_time": "2023-08-04T23:05:25.511692800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(120,)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:08:29.316651200Z",
     "start_time": "2023-08-04T23:08:29.303142400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtest\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "test[:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:04:36.313564800Z",
     "start_time": "2023-08-04T23:04:36.301534700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
