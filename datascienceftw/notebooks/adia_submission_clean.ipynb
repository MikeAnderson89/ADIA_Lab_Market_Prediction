{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### FUNCTIONS #####\n",
    "\n",
    "\n",
    "def pairwise_combinations_with_ids_and_values(X):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "    X_pairs = []\n",
    "    X_pair_ids = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        date_i = X[i, 0]\n",
    "        same_date_indices = [j for j in range(i + 1, n_samples) if X[j, 0] == date_i]\n",
    "        for j in same_date_indices:\n",
    "            pair_key = tuple(sorted((i, j)))  # Create a unique key for the pair\n",
    "            if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                X_pair = np.array([X[i, 2:], X[j, 2:]])\n",
    "                X_pair_id = np.array([X[i, :2], X[j, :2]])\n",
    "                generated_pairs.add(pair_key)\n",
    "                X_pairs.append(X_pair)\n",
    "                X_pair_ids.append(X_pair_id)\n",
    "\n",
    "    return np.array(X_pairs, dtype='float32'), np.array(X_pair_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_generator5(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    generated_pairs = set()  # To store generated pairs and avoid duplicates\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)  # Shuffle indices to create random batches\n",
    "        batch_X = X[indices]\n",
    "        batch_y = y[indices]\n",
    "\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Create a dictionary to store indices for each date\n",
    "        date_indices = defaultdict(list)\n",
    "        for i in range(n_samples):\n",
    "            date_indices[batch_X[i, 0]].append(i)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            date_i = batch_X[i, 0]\n",
    "            same_date_indices = date_indices[date_i]\n",
    "            for j in same_date_indices:\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                pair_key = (i, j)  # Use indices directly as a unique key\n",
    "                if pair_key not in generated_pairs:  # Check if pair is already generated\n",
    "                    X_pair = np.array([batch_X[i, 2:], batch_X[j, 2:]])\n",
    "                    y_pair = 1 if batch_y[i, 2] > batch_y[j, 2] else 0\n",
    "                    X_batch.append(X_pair)\n",
    "                    y_batch.append(y_pair)\n",
    "                    generated_pairs.add(pair_key)\n",
    "\n",
    "                    if len(X_batch) == batch_size:\n",
    "                        yield np.array(X_batch, dtype='float32'), np.array(y_batch, dtype='float32').reshape(-1,1)\n",
    "                        X_batch = []\n",
    "                        y_batch = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### TRAIN #####\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"../resources\") -> None:\n",
    "    max_date = X_train['date'].max()\n",
    "    min_date = 150\n",
    "    X_train_orig = X_train[(X_train['date'] < max_date) & (X_train['date'] > min_date)]\n",
    "    X_test = X_train[X_train['date'] == max_date]\n",
    "    y_train_orig = y_train[(y_train['date'] < max_date) & (y_train['date'] > min_date)]\n",
    "    y_test = y_train[y_train['date'] == max_date]\n",
    "\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_ids = np.asarray(X_train_orig[['date', 'id']])\n",
    "    X_scale_pca = X_train_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.fit_transform(X_scale_pca)\n",
    "\n",
    "\n",
    "    X_test_ids = np.asarray(X_test[['date', 'id']])\n",
    "    X_test_scale_pca = X_test.drop(columns=['date', 'id'])\n",
    "    X_test_scale_pca = scaler.transform(X_test_scale_pca)\n",
    "\n",
    "    #PCA\n",
    "    n_components = 40\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(X_scale_pca)\n",
    "    X_train_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    pca_features_test = pca.transform(X_test_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_test_ids, pca_features_test), axis=1)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    #Save out Scaler and PCA\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "\n",
    "    date_list = list(set(X_train_orig['date']))\n",
    "\n",
    "    batch_size = 500\n",
    "    train_generator = pairwise_generator5(X_train_concat, y_train, batch_size)\n",
    "    test_generator= pairwise_generator5(X_test_concat, y_test, batch_size)\n",
    "\n",
    "    print(X_train_concat.shape)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    model_pathname = Path('../resources') / \"model.keras\"\n",
    "\n",
    "    if model_pathname.is_file():\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch = 1000,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps = 500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        #Neural Network Model\n",
    "        mc = ModelCheckpoint(model_pathname, monitor='val_loss', mode='min', verbose=1, save_best_only=False)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None)\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(200, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01), input_shape=(2, (X_train_concat.shape[1] - 2))),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            #keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(250, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            # keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            # keras.layers.Dense(100, activation='relu', kernel_initializer='lecun_normal'),\n",
    "            # keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization\n",
    "            keras.layers.Dense(25, activation='relu', kernel_initializer='lecun_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # Adding dropout regularization,\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_normal')\n",
    "        ])\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=30,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=500,\n",
    "            callbacks=[mc, early_stopping],\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        model.save(model_pathname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Finished All Training\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def infer(X_test: pd.DataFrame, model_directory_path: str = \"../resources\") -> pd.DataFrame:\n",
    "    X_test_orig = X_test.copy()\n",
    "\n",
    "    # Load Scaler\n",
    "    with open(Path(model_directory_path) / 'scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "\n",
    "    # Load PCA\n",
    "    with open(Path(model_directory_path) / 'pca.pkl', 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "\n",
    "    # Scaling\n",
    "    X_ids = np.asarray(X_test_orig[['date', 'id']])\n",
    "    X_scale_pca = X_test_orig.drop(columns=['date', 'id'])\n",
    "    X_scale_pca = scaler.transform(X_scale_pca)\n",
    "\n",
    "    # PCA\n",
    "    pca_features = pca.transform(X_scale_pca)\n",
    "    X_test_concat = np.concatenate((X_ids, pca_features), axis=1)\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['date', 'id', 'value'])\n",
    "\n",
    "    # Load Model\n",
    "    model_pathname = Path(model_directory_path) / \"model.keras\"\n",
    "    model = load_model(model_pathname)\n",
    "\n",
    "    # Pairwise Transformation using the pairwise generator\n",
    "    batch_size = 1000\n",
    "    X_test, X_test_ids = pairwise_combinations_with_ids_and_values(X_test_concat)\n",
    "\n",
    "    print(\"Predicting for Test Data\")\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame({'id': X_test_ids[:, 0, 1].flatten(), 'date': X_test_ids[:, 0, 0].flatten(), 'value': preds.flatten()})\n",
    "    preds_df = preds_df.groupby(['date', 'id']).mean().reset_index()\n",
    "\n",
    "    result_df = pd.merge(X_test_orig, preds_df, on=['id', 'date'], how='left')\n",
    "    result_df['value'] = result_df['value'].fillna(result_df['value'].mean())\n",
    "\n",
    "    minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "    result_df['value'] = minmax.fit_transform(result_df[['value']])\n",
    "\n",
    "    print(\"Finished predicting Test Data\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../data/X_train.parquet')\n",
    "y_train = pd.read_parquet('../data/y_train.parquet')\n",
    "X_test = pd.read_parquet('../data/X_test.parquet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = infer(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
